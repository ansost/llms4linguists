{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4dPSnSw59x9"
      },
      "source": [
        "# Morphological Inflection in English\n",
        "\n",
        "```\n",
        "sleep <V;PST> -> slept\n",
        "```\n",
        "\n",
        "You can find out more about the annotation schema used for the morphosyntactic specification of the target forms [here](https://unimorph.github.io/doc/unimorph-schema.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yui_SWs9mXx-",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d71d481-d131-48ca-c6e2-ab1c58c59c4c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Downloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.1.1\n",
            "    Uninstalling accelerate-1.1.1:\n",
            "      Successfully uninstalled accelerate-1.1.1\n",
            "Successfully installed accelerate-1.2.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "Successfully installed tokenizers-0.21.0 transformers-4.47.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U evaluate\n",
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download datasets"
      ],
      "metadata": {
        "id": "OQivu7UUjc29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "R_dknIv5LIsz",
        "outputId": "ecd3f188-57a7-4d6c-e495-ca3239614952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully downloaded 1000 lines from eng_1000.train\n",
            "✓ Successfully downloaded 454 lines from eng.dev\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from typing import List, Dict\n",
        "from requests.exceptions import RequestException\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "\n",
        "def download_data(url: str, max_retries: int = 3, timeout: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Download and process data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to download data from\n",
        "        max_retries: Maximum number of retry attempts\n",
        "        timeout: Timeout in seconds for the request\n",
        "\n",
        "    Returns:\n",
        "        List of strings, each representing a line from the downloaded data\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            data = response.text.strip().split('\\n')\n",
        "            print(f\"✓ Successfully downloaded {len(data)} lines from {url.split('/')[-1]}\")\n",
        "            return data\n",
        "\n",
        "        except RequestException as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                display(HTML(f\"<b style='color: red'>Error downloading {url.split('/')[-1]}: {str(e)}</b>\"))\n",
        "                raise\n",
        "            print(f\"⚠️ Attempt {attempt + 1} failed, retrying...\")\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "# Data URLs\n",
        "train_data_path = 'https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng_1000.train'\n",
        "dev_data_path = 'https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng.dev'\n",
        "\n",
        "# Download and store data\n",
        "data = {}\n",
        "try:\n",
        "    data['train'] = download_data(train_data_path)\n",
        "    data['dev'] = download_data(dev_data_path)\n",
        "except Exception as e:\n",
        "    display(HTML(f\"<b style='color: red'>Failed to load datasets: {str(e)}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset statistics with detailed formatting\n",
        "def display_data_summary(data: dict) -> None:\n",
        "    \"\"\"\n",
        "    Display summary statistics and sample data in a formatted way.\n",
        "\n",
        "    Args:\n",
        "        data: Dictionary containing 'train' and 'dev' datasets\n",
        "    \"\"\"\n",
        "    # Print dataset sizes with formatted output\n",
        "    print(f\"\\n📊 Dataset Statistics:\")\n",
        "    print(f\"  • Training samples: {len(data['train']):,}\")\n",
        "    print(f\"  • Development samples: {len(data['dev']):,}\")\n",
        "\n",
        "    sample = data['train'][0]\n",
        "    print(f\"\\n📝 Sample Data Format:\", sample)\n",
        "\n",
        "# Display the summary\n",
        "display_data_summary(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KAcZMCphjtQh",
        "outputId": "79365448-6d78-410d-c11b-34480a4dc39a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Dataset Statistics:\n",
            "  • Training samples: 1,000\n",
            "  • Development samples: 454\n",
            "\n",
            "📝 Sample Data Format: take\ttook\tV;PST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "pK2D5WR9kSnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1tJreU_6Y2Q",
        "outputId": "e97ad751-5146-49b3-e2cd-807aad5c0701",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Preprocessed sample:\n",
            "  ('take <V><PST>', 'took')\n",
            "\n",
            "📊 Vocabulary Statistics:\n",
            "  • Unique characters: 26\n",
            "  • Unique tags: 2\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import regex\n",
        "from typing import List, Tuple, Set, Dict\n",
        "\n",
        "def parse_tag(tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Parse a tag string by removing unwanted characters and formatting each tag.\n",
        "\n",
        "    Args:\n",
        "        tag: Input tag string containing tags separated by delimiters\n",
        "\n",
        "    Returns:\n",
        "        Formatted string with each tag wrapped in angle brackets\n",
        "    \"\"\"\n",
        "    tag = re.sub(r\"\\)|\\(|,|;\", ' ', tag).split()\n",
        "    return ''.join(['<{}>'.format(t) for t in tag])\n",
        "\n",
        "\n",
        "def preprocess_data(raw_data: List[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Preprocess raw data by parsing each line into formatted input-target pairs.\n",
        "\n",
        "    Args:\n",
        "        raw_data: List of raw data lines (lemma, target, tag separated by tabs)\n",
        "\n",
        "    Returns:\n",
        "        List of tuples containing (formatted_input, target)\n",
        "    \"\"\"\n",
        "    preprocessed_data = []\n",
        "    for line in raw_data:\n",
        "        try:\n",
        "            lemma, target, tag = line.split('\\t')\n",
        "            formatted_input = f\"{lemma} {parse_tag(tag)}\"\n",
        "            preprocessed_data.append((formatted_input, target))\n",
        "        except ValueError as e:\n",
        "            print(f\"⚠️ Skipping malformed line: {line} (Error: {e})\")\n",
        "\n",
        "    return preprocessed_data\n",
        "\n",
        "def extract_vocab(data: List[Tuple[str, str]]) -> Tuple[Set[str], List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Extract vocabulary, tags, and character mappings from preprocessed data.\n",
        "\n",
        "    Args:\n",
        "        data: List of preprocessed (input, target) pairs\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Set of unique characters\n",
        "        - List of unique tags\n",
        "        - Dictionary mapping characters to indices\n",
        "    \"\"\"\n",
        "    # Extract characters from lemmas and targets\n",
        "    chars = set(''.join([d[0].split()[0] + d[1] for d in data]))\n",
        "\n",
        "    # Create character to index mapping\n",
        "    char2id = {char: idx for idx, char in enumerate(sorted(chars))}\n",
        "\n",
        "    # Extract unique tags\n",
        "    tags = list(set(regex.findall(r\"<[A-Za-z0-9]+>\", ' '.join(d[0] for d in data))))\n",
        "\n",
        "    return chars, tags, char2id\n",
        "\n",
        "# Process the data\n",
        "data['train'] = preprocess_data(data['train'])\n",
        "data['dev'] = preprocess_data(data['dev'])\n",
        "\n",
        "# Display sample of preprocessed data\n",
        "print(f\"\\n📝 Preprocessed sample:\")\n",
        "print(f\"  {data['train'][0]}\")\n",
        "\n",
        "# Extract vocabulary and mappings\n",
        "chars, tags, char2id = extract_vocab(data['train'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "XO-qTiG5LIs0",
        "outputId": "cceed4e9-6dbc-4f3d-a788-7e2f68633a9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization Example:\n",
            "Input text: shut <V><PST>\n",
            "Tokens: ['s', 'h', 'u', 't', ' ', '<V>', '<PST>']\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, List, Union\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "@dataclass\n",
        "class TokenizerConfig:\n",
        "    \"\"\"Configuration for CustomTokenizer\"\"\"\n",
        "    bos_token: str = \"<s>\"\n",
        "    eos_token: str = \"</s>\"\n",
        "    unk_token: str = \"<unk>\"\n",
        "    pad_token: str = \"<pad>\"\n",
        "    max_len: int = 512\n",
        "\n",
        "class CustomTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    Custom tokenizer for character-level tokenization with special handling for morphosyntactic features.\n",
        "\n",
        "    Processes input and output word forms character by character while treating morphosyntactic\n",
        "    features as special atomic tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Dict[str, int],\n",
        "        additional_special_tokens: Optional[List[str]] = None,\n",
        "        max_len: int = 512,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocab: Dictionary mapping characters to their IDs\n",
        "            additional_special_tokens: List of special tokens (e.g., morphological tags)\n",
        "            max_len: Maximum sequence length\n",
        "        \"\"\"\n",
        "        config = TokenizerConfig()\n",
        "\n",
        "        # Initialize token mappings\n",
        "        self.__token_ids = vocab.copy()\n",
        "        self.__id_tokens = {v: k for k, v in vocab.items()}\n",
        "\n",
        "        # Process special tokens\n",
        "        special_tokens = {\n",
        "            'pad_token': self._create_added_token(config.pad_token),\n",
        "            'bos_token': self._create_added_token(config.bos_token),\n",
        "            'eos_token': self._create_added_token(config.eos_token),\n",
        "            'unk_token': self._create_added_token(config.unk_token)\n",
        "        }\n",
        "\n",
        "        # Initialize special token decoder\n",
        "        self._added_tokens_decoder = {\n",
        "            i: token for i, token in enumerate(special_tokens.values())\n",
        "        }\n",
        "        self.offset = len(self._added_tokens_decoder)\n",
        "\n",
        "        super().__init__(\n",
        "            **special_tokens,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            max_len=max_len,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_added_token(token: Union[str, AddedToken]) -> AddedToken:\n",
        "        \"\"\"Create an AddedToken with consistent settings.\"\"\"\n",
        "        return AddedToken(token, lstrip=False, rstrip=False) if isinstance(token, str) else token\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Get the size of the vocabulary.\"\"\"\n",
        "        return len(self.__token_ids)\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        \"\"\"Get the full vocabulary including special tokens.\"\"\"\n",
        "        vocab = {\n",
        "            self.convert_ids_to_tokens(i): i\n",
        "            for i in range(self.vocab_size + self.offset)\n",
        "        }\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _add_eos(self, token_ids: List[int]) -> List[int]:\n",
        "        \"\"\"Add end-of-sequence token to the token list.\"\"\"\n",
        "        return token_ids + [self.eos_token_id]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Create token type IDs for single or paired sequences.\"\"\"\n",
        "        total_length = len(token_ids_0) + 1  # +1 for EOS\n",
        "        if token_ids_1:\n",
        "            total_length += len(token_ids_1) + 1\n",
        "        return [0] * total_length\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Build model inputs by adding special tokens.\"\"\"\n",
        "        token_ids_0 = self._add_eos(token_ids_0)\n",
        "        if token_ids_1 is None:\n",
        "            return token_ids_0\n",
        "        return token_ids_0 + self._add_eos(token_ids_1)\n",
        "\n",
        "    def _tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        \"\"\"Tokenize text into characters.\"\"\"\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token to its ID.\"\"\"\n",
        "        return (self.__token_ids.get(token, self.unk_token_id - self.offset)\n",
        "                + self.offset)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        \"\"\"Convert an ID back to its token.\"\"\"\n",
        "        adjusted_index = index - self.offset\n",
        "        return self.__id_tokens.get(adjusted_index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Convert tokens back to string.\"\"\"\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocabulary(self,\n",
        "                       save_directory: str,\n",
        "                       filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        \"\"\"Save the vocabulary to a file.\"\"\"\n",
        "        prefix = filename_prefix or \"\"\n",
        "        vocab_path = Path(save_directory) / f\"{prefix}vocab.json\"\n",
        "\n",
        "        with vocab_path.open('w', encoding='utf-8') as f:\n",
        "            json.dump(self.__token_ids, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return (str(vocab_path),)\n",
        "\n",
        "# Initialize tokenizer with vocabulary and tags\n",
        "tokenizer = CustomTokenizer(char2id, additional_special_tokens=tags, max_len=100)\n",
        "\n",
        "# Display example tokenization\n",
        "sample_idx = 54\n",
        "sample_text = data['train'][sample_idx][0]\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(f\"Tokenization Example:\")\n",
        "print(f\"Input text: {sample_text}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training T5 model ([Raffel et al.](https://arxiv.org/pdf/1910.10683))"
      ],
      "metadata": {
        "id": "97wGm05szfh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "5kmN6C1iAxlL"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Config, T5ForConditionalGeneration\n",
        "from typing import Optional\n",
        "\n",
        "def create_t5_model(\n",
        "    tokenizer,\n",
        "    d_ff: int = 1024,\n",
        "    d_model: int = 256,\n",
        "    num_layers: int = 4,\n",
        "    num_heads: int = 4,\n",
        "    dropout_rate: float = 0.2,\n",
        "    max_new_tokens: int = 32,\n",
        "    d_kv: Optional[int] = None\n",
        ") -> T5ForConditionalGeneration:\n",
        "    \"\"\"\n",
        "    Creates and configures a T5 model for morphological generation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Custom tokenizer for the model\n",
        "        d_ff: Dimension of the feed-forward layer\n",
        "        d_model: Dimension of the model embeddings\n",
        "        num_layers: Number of encoder and decoder layers\n",
        "        num_heads: Number of attention heads\n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        d_kv: Dimension of key and value vectors (defaults to d_model // num_heads)\n",
        "\n",
        "    Returns:\n",
        "        Configured T5ForConditionalGeneration model\n",
        "    \"\"\"\n",
        "    # Calculate default d_kv if not provided\n",
        "    if d_kv is None:\n",
        "        d_kv = d_model // num_heads\n",
        "\n",
        "    # Configure model architecture\n",
        "    config = T5Config(\n",
        "        d_ff=d_ff,\n",
        "        d_model=d_model,\n",
        "        d_kv=d_kv,\n",
        "        num_layers=num_layers,\n",
        "        num_decoder_layers=num_layers,  # Match encoder layers\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate,\n",
        "        vocab_size=len(tokenizer),\n",
        "        is_encoder_decoder=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = T5ForConditionalGeneration(config)\n",
        "\n",
        "    # Configure generation parameters\n",
        "    model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "    model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "    model.generation_config.max_new_tokens = max_new_tokens\n",
        "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.generation_config.no_repeat_ngram_size = 2\n",
        "    model.generation_config.length_penalty = 1.0\n",
        "    model.generation_config.num_beams = 10\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the model with default parameters\n",
        "model = create_t5_model(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "CWQTI68gLIs1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from typing import List, Dict, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for morphological generation task.\n",
        "    Handles tokenization of input lemmas and target forms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: List[Tuple[str, str]],\n",
        "        tokenizer,\n",
        "        max_length: int = 128\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "\n",
        "        Args:\n",
        "            data: List of (input, target) string pairs\n",
        "            tokenizer: Tokenizer for encoding text\n",
        "            max_length: Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return dataset size.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get tokenized item from dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with input_ids and labels tensors\n",
        "        \"\"\"\n",
        "        input_text, target_text = self.data[idx]\n",
        "\n",
        "        # Tokenize input and target\n",
        "        model_inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = self.tokenizer(\n",
        "            target_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": model_inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "def postprocess_data(\n",
        "    token_ids: Union[np.ndarray, torch.Tensor],\n",
        "    tokenizer\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Post-process token IDs into readable text.\n",
        "\n",
        "    Args:\n",
        "        token_ids: Array/Tensor of token IDs\n",
        "        tokenizer: Tokenizer for decoding\n",
        "\n",
        "    Returns:\n",
        "        List of decoded strings with special tokens removed\n",
        "    \"\"\"\n",
        "    # Convert torch tensor to numpy if needed\n",
        "    if isinstance(token_ids, torch.Tensor):\n",
        "        token_ids = token_ids.cpu().numpy()\n",
        "\n",
        "    # Replace padding indices\n",
        "    token_ids = np.where(token_ids != -100, token_ids, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode tokens to strings\n",
        "    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "PixE9lVPLIs1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Evaluation metrics for morphological generation task.\n",
        "Includes exact match scoring and detailed error analysis.\n",
        "\"\"\"\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import Levenshtein\n",
        "from collections import defaultdict\n",
        "\n",
        "@dataclass\n",
        "class PredictionExample:\n",
        "    \"\"\"Store prediction examples for analysis\"\"\"\n",
        "    input: str\n",
        "    prediction: str\n",
        "    target: str\n",
        "    is_correct: bool\n",
        "    edit_distance: int\n",
        "\n",
        "class MetricsComputer:\n",
        "    \"\"\"Compute and analyze model predictions\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, sample_size: int = 15):\n",
        "        \"\"\"\n",
        "        Initialize metrics computer.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: Tokenizer for decoding predictions\n",
        "            sample_size: Number of random examples to sample for analysis\n",
        "        \"\"\"\n",
        "        self.metric = evaluate.load(\"exact_match\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_size = sample_size\n",
        "        self.error_stats = defaultdict(int)\n",
        "\n",
        "    def compute_edit_distance(self, pred: str, target: str) -> int:\n",
        "        \"\"\"Compute Levenshtein distance between prediction and target\"\"\"\n",
        "        return Levenshtein.distance(pred, target)\n",
        "\n",
        "    def analyze_predictions(\n",
        "        self,\n",
        "        decoded_preds: List[str],\n",
        "        decoded_labels: List[str],\n",
        "        inputs: List[str] = None\n",
        "    ) -> List[PredictionExample]:\n",
        "        \"\"\"\n",
        "        Analyze a sample of predictions.\n",
        "\n",
        "        Args:\n",
        "            decoded_preds: List of model predictions\n",
        "            decoded_labels: List of target labels\n",
        "            inputs: Optional list of input texts\n",
        "\n",
        "        Returns:\n",
        "            List of PredictionExample objects\n",
        "        \"\"\"\n",
        "        sample_indices = random.sample(range(len(decoded_preds)),\n",
        "                                     min(self.sample_size, len(decoded_preds)))\n",
        "\n",
        "        examples = []\n",
        "        for idx in sample_indices:\n",
        "            pred = decoded_preds[idx]\n",
        "            target = decoded_labels[idx]\n",
        "            input_text = inputs[idx] if inputs else \"\"\n",
        "\n",
        "            example = PredictionExample(\n",
        "                input=input_text,\n",
        "                prediction=pred,\n",
        "                target=target,\n",
        "                is_correct=pred == target,\n",
        "                edit_distance=self.compute_edit_distance(pred, target)\n",
        "            )\n",
        "            examples.append(example)\n",
        "\n",
        "            # Track error types\n",
        "            if not example.is_correct:\n",
        "                self.error_stats['total_errors'] += 1\n",
        "                if len(pred) != len(target):\n",
        "                    self.error_stats['length_mismatch'] += 1\n",
        "                if pred == target[::-1]:\n",
        "                    self.error_stats['reversed'] += 1\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def compute_metrics(self, eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute evaluation metrics for predictions.\n",
        "\n",
        "        Args:\n",
        "            eval_preds: Tuple of (predictions, labels) arrays\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of metric scores\n",
        "        \"\"\"\n",
        "        preds, labels = eval_preds\n",
        "        decoded_preds = postprocess_data(preds, self.tokenizer)\n",
        "        decoded_labels = postprocess_data(labels, self.tokenizer)\n",
        "\n",
        "        # Compute exact match score\n",
        "        exact_match = self.metric.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels\n",
        "        )\n",
        "\n",
        "        # Analyze sample predictions\n",
        "        examples = self.analyze_predictions(decoded_preds, decoded_labels)\n",
        "\n",
        "        # Compute additional metrics\n",
        "        edit_distances = [ex.edit_distance for ex in examples]\n",
        "\n",
        "        metrics = {\n",
        "            \"exact_match\": exact_match[\"exact_match\"],\n",
        "            \"avg_edit_distance\": np.mean(edit_distances),\n",
        "            \"max_edit_distance\": max(edit_distances),\n",
        "            \"error_rate\": self.error_stats['total_errors'] / len(examples)\n",
        "        }\n",
        "\n",
        "        # Print analysis examples\n",
        "        self._print_analysis(examples)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _print_analysis(self, examples: List[PredictionExample]) -> None:\n",
        "        \"\"\"Print detailed analysis of prediction examples\"\"\"\n",
        "        print(\"\\n📊 Prediction Analysis:\")\n",
        "        print(f\"Analyzing {len(examples)} random examples...\")\n",
        "        print(\"\\nDetailed Examples:\")\n",
        "\n",
        "        for i, ex in enumerate(examples, 1):\n",
        "            status = \"✓\" if ex.is_correct else \"✗\"\n",
        "            print(f\"\\n{i}. {status} Input: {ex.input}\")\n",
        "            print(f\"   Prediction: {ex.prediction}\")\n",
        "            print(f\"   Target: {ex.target}\")\n",
        "            print(f\"   Edit Distance: {ex.edit_distance}\")\n",
        "\n",
        "        print(\"\\nError Statistics:\")\n",
        "        for error_type, count in self.error_stats.items():\n",
        "            print(f\"  • {error_type}: {count}\")\n",
        "\n",
        "# Initialize metrics computer\n",
        "metrics_computer = MetricsComputer(tokenizer)\n",
        "\n",
        "# Use for evaluation\n",
        "compute_metrics = metrics_computer.compute_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "R0n-bgxSH8HP",
        "outputId": "ef3a18ef-3100-4fdc-9b1e-84dbe5e79bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 07:23, Epoch 125/125]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>Avg Edit Distance</th>\n",
              "      <th>Max Edit Distance</th>\n",
              "      <th>Error Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.119600</td>\n",
              "      <td>1.191470</td>\n",
              "      <td>0.151982</td>\n",
              "      <td>2.266667</td>\n",
              "      <td>6</td>\n",
              "      <td>15.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.196100</td>\n",
              "      <td>1.175861</td>\n",
              "      <td>0.151982</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>16.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.183400</td>\n",
              "      <td>1.162995</td>\n",
              "      <td>0.167401</td>\n",
              "      <td>2.466667</td>\n",
              "      <td>5</td>\n",
              "      <td>17.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.152100</td>\n",
              "      <td>1.146829</td>\n",
              "      <td>0.176211</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>4</td>\n",
              "      <td>18.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.121300</td>\n",
              "      <td>1.138121</td>\n",
              "      <td>0.176211</td>\n",
              "      <td>2.733333</td>\n",
              "      <td>6</td>\n",
              "      <td>19.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✗ Input: N/A\n",
            "   Prediction: iled\n",
            "   Target: lay\n",
            "   Edit Distance: 3\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: caked\n",
            "   Target: jacked\n",
            "   Edit Distance: 2\n",
            "\n",
            "3. ✓ Input: N/A\n",
            "   Prediction: shoveled\n",
            "   Target: shoveled\n",
            "   Edit Distance: 0\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: shiged\n",
            "   Target: sighed\n",
            "   Edit Distance: 2\n",
            "\n",
            "5. ✗ Input: N/A\n",
            "   Prediction: stramed\n",
            "   Target: mastered\n",
            "   Edit Distance: 5\n",
            "\n",
            "6. ✗ Input: N/A\n",
            "   Prediction: grated\n",
            "   Target: regurgitated\n",
            "   Edit Distance: 6\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: inted\n",
            "   Target: invited\n",
            "   Edit Distance: 2\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: shoped\n",
            "   Target: shod\n",
            "   Edit Distance: 2\n",
            "\n",
            "9. ✗ Input: N/A\n",
            "   Prediction: bamed\n",
            "   Target: bammed\n",
            "   Edit Distance: 1\n",
            "\n",
            "10. ✗ Input: N/A\n",
            "   Prediction: dasted\n",
            "   Target: devastated\n",
            "   Edit Distance: 4\n",
            "\n",
            "11. ✓ Input: N/A\n",
            "   Prediction: hooked\n",
            "   Target: hooked\n",
            "   Edit Distance: 0\n",
            "\n",
            "12. ✓ Input: N/A\n",
            "   Prediction: hated\n",
            "   Target: hated\n",
            "   Edit Distance: 0\n",
            "\n",
            "13. ✗ Input: N/A\n",
            "   Prediction: groned\n",
            "   Target: ignored\n",
            "   Edit Distance: 3\n",
            "\n",
            "14. ✗ Input: N/A\n",
            "   Prediction: dited\n",
            "   Target: dieted\n",
            "   Edit Distance: 1\n",
            "\n",
            "15. ✗ Input: N/A\n",
            "   Prediction: rastined\n",
            "   Target: restrained\n",
            "   Edit Distance: 3\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 235\n",
            "  • length_mismatch: 203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✓ Input: N/A\n",
            "   Prediction: punched\n",
            "   Target: punched\n",
            "   Edit Distance: 0\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: shased\n",
            "   Target: sashayed\n",
            "   Edit Distance: 3\n",
            "\n",
            "3. ✗ Input: N/A\n",
            "   Prediction: furthed\n",
            "   Target: furthered\n",
            "   Edit Distance: 2\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: strated\n",
            "   Target: starred\n",
            "   Edit Distance: 3\n",
            "\n",
            "5. ✗ Input: N/A\n",
            "   Prediction: waded\n",
            "   Target: waddled\n",
            "   Edit Distance: 2\n",
            "\n",
            "6. ✗ Input: N/A\n",
            "   Prediction: prished\n",
            "   Target: whispered\n",
            "   Edit Distance: 5\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: briged\n",
            "   Target: bridged\n",
            "   Edit Distance: 1\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: magled\n",
            "   Target: gleamed\n",
            "   Edit Distance: 5\n",
            "\n",
            "9. ✗ Input: N/A\n",
            "   Prediction: claned\n",
            "   Target: cleaned\n",
            "   Edit Distance: 1\n",
            "\n",
            "10. ✗ Input: N/A\n",
            "   Prediction: compliced\n",
            "   Target: compiled\n",
            "   Edit Distance: 2\n",
            "\n",
            "11. ✓ Input: N/A\n",
            "   Prediction: cawed\n",
            "   Target: cawed\n",
            "   Edit Distance: 0\n",
            "\n",
            "12. ✗ Input: N/A\n",
            "   Prediction: panded\n",
            "   Target: kidnapped\n",
            "   Edit Distance: 6\n",
            "\n",
            "13. ✓ Input: N/A\n",
            "   Prediction: glowed\n",
            "   Target: glowed\n",
            "   Edit Distance: 0\n",
            "\n",
            "14. ✓ Input: N/A\n",
            "   Prediction: hated\n",
            "   Target: hated\n",
            "   Edit Distance: 0\n",
            "\n",
            "15. ✓ Input: N/A\n",
            "   Prediction: wet\n",
            "   Target: wet\n",
            "   Edit Distance: 0\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 245\n",
            "  • length_mismatch: 212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✗ Input: N/A\n",
            "   Prediction: houroned\n",
            "   Target: honoured\n",
            "   Edit Distance: 4\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: sliped\n",
            "   Target: spilled\n",
            "   Edit Distance: 3\n",
            "\n",
            "3. ✗ Input: N/A\n",
            "   Prediction: bused\n",
            "   Target: subdued\n",
            "   Edit Distance: 4\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: goraned\n",
            "   Target: organized\n",
            "   Edit Distance: 4\n",
            "\n",
            "5. ✗ Input: N/A\n",
            "   Prediction: scoked\n",
            "   Target: socked\n",
            "   Edit Distance: 2\n",
            "\n",
            "6. ✗ Input: N/A\n",
            "   Prediction: baled\n",
            "   Target: babbled\n",
            "   Edit Distance: 2\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: flafed\n",
            "   Target: fell\n",
            "   Edit Distance: 5\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: slaised\n",
            "   Target: sailed\n",
            "   Edit Distance: 2\n",
            "\n",
            "9. ✗ Input: N/A\n",
            "   Prediction: wated\n",
            "   Target: waited\n",
            "   Edit Distance: 1\n",
            "\n",
            "10. ✓ Input: N/A\n",
            "   Prediction: kicked\n",
            "   Target: kicked\n",
            "   Edit Distance: 0\n",
            "\n",
            "11. ✗ Input: N/A\n",
            "   Prediction: barolted\n",
            "   Target: elaborated\n",
            "   Edit Distance: 5\n",
            "\n",
            "12. ✗ Input: N/A\n",
            "   Prediction: rastined\n",
            "   Target: restrained\n",
            "   Edit Distance: 3\n",
            "\n",
            "13. ✗ Input: N/A\n",
            "   Prediction: mached\n",
            "   Target: matched\n",
            "   Edit Distance: 1\n",
            "\n",
            "14. ✗ Input: N/A\n",
            "   Prediction: mamed\n",
            "   Target: maxed\n",
            "   Edit Distance: 1\n",
            "\n",
            "15. ✓ Input: N/A\n",
            "   Prediction: ditched\n",
            "   Target: ditched\n",
            "   Edit Distance: 0\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 258\n",
            "  • length_mismatch: 222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✗ Input: N/A\n",
            "   Prediction: diged\n",
            "   Target: dug\n",
            "   Edit Distance: 3\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: structed\n",
            "   Target: structured\n",
            "   Edit Distance: 2\n",
            "\n",
            "3. ✗ Input: N/A\n",
            "   Prediction: dasted\n",
            "   Target: devastated\n",
            "   Edit Distance: 4\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: fruthed\n",
            "   Target: furthered\n",
            "   Edit Distance: 4\n",
            "\n",
            "5. ✗ Input: N/A\n",
            "   Prediction: ked\n",
            "   Target: keyed\n",
            "   Edit Distance: 2\n",
            "\n",
            "6. ✗ Input: N/A\n",
            "   Prediction: pized\n",
            "   Target: zipped\n",
            "   Edit Distance: 3\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: snded\n",
            "   Target: sent\n",
            "   Edit Distance: 4\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: prigled\n",
            "   Target: privileged\n",
            "   Edit Distance: 4\n",
            "\n",
            "9. ✓ Input: N/A\n",
            "   Prediction: munched\n",
            "   Target: munched\n",
            "   Edit Distance: 0\n",
            "\n",
            "10. ✗ Input: N/A\n",
            "   Prediction: shablisted\n",
            "   Target: established\n",
            "   Edit Distance: 3\n",
            "\n",
            "11. ✗ Input: N/A\n",
            "   Prediction: manded\n",
            "   Target: demanded\n",
            "   Edit Distance: 2\n",
            "\n",
            "12. ✗ Input: N/A\n",
            "   Prediction: wrinked\n",
            "   Target: wrinkled\n",
            "   Edit Distance: 1\n",
            "\n",
            "13. ✗ Input: N/A\n",
            "   Prediction: sloved\n",
            "   Target: solved\n",
            "   Edit Distance: 2\n",
            "\n",
            "14. ✗ Input: N/A\n",
            "   Prediction: barbed\n",
            "   Target: jabbered\n",
            "   Edit Distance: 4\n",
            "\n",
            "15. ✗ Input: N/A\n",
            "   Prediction: showed\n",
            "   Target: shadowed\n",
            "   Edit Distance: 2\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 272\n",
            "  • length_mismatch: 235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✗ Input: N/A\n",
            "   Prediction: hared\n",
            "   Target: heard\n",
            "   Edit Distance: 2\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: rented\n",
            "   Target: entered\n",
            "   Edit Distance: 3\n",
            "\n",
            "3. ✗ Input: N/A\n",
            "   Prediction: grined\n",
            "   Target: grinned\n",
            "   Edit Distance: 1\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: waned\n",
            "   Target: yawned\n",
            "   Edit Distance: 2\n",
            "\n",
            "5. ✗ Input: N/A\n",
            "   Prediction: witched\n",
            "   Target: twitched\n",
            "   Edit Distance: 1\n",
            "\n",
            "6. ✓ Input: N/A\n",
            "   Prediction: fenced\n",
            "   Target: fenced\n",
            "   Edit Distance: 0\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: mprented\n",
            "   Target: experimented\n",
            "   Edit Distance: 5\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: cofered\n",
            "   Target: forced\n",
            "   Edit Distance: 4\n",
            "\n",
            "9. ✗ Input: N/A\n",
            "   Prediction: ivicled\n",
            "   Target: civilized\n",
            "   Edit Distance: 4\n",
            "\n",
            "10. ✗ Input: N/A\n",
            "   Prediction: mized\n",
            "   Target: jimmied\n",
            "   Edit Distance: 4\n",
            "\n",
            "11. ✗ Input: N/A\n",
            "   Prediction: worthed\n",
            "   Target: threw\n",
            "   Edit Distance: 5\n",
            "\n",
            "12. ✓ Input: N/A\n",
            "   Prediction: created\n",
            "   Target: created\n",
            "   Edit Distance: 0\n",
            "\n",
            "13. ✗ Input: N/A\n",
            "   Prediction: shited\n",
            "   Target: shifted\n",
            "   Edit Distance: 1\n",
            "\n",
            "14. ✗ Input: N/A\n",
            "   Prediction: corationded\n",
            "   Target: coordinated\n",
            "   Edit Distance: 6\n",
            "\n",
            "15. ✗ Input: N/A\n",
            "   Prediction: bated\n",
            "   Target: beat\n",
            "   Edit Distance: 3\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 285\n",
            "  • length_mismatch: 246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=1.1623030591011048, metrics={'train_runtime': 444.0954, 'train_samples_per_second': 288.226, 'train_steps_per_second': 2.252, 'total_flos': 74702934736896.0, 'train_loss': 1.1623030591011048, 'epoch': 125.0})"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "import warnings\n",
        "from transformers import utils\n",
        "\n",
        "# Monkey patch the deprecated warning in transformers utils\n",
        "utils.deprecation_warning = lambda *args, **kwargs: None\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from pathlib import Path\n",
        "\n",
        "# Create datasets with proper batch size\n",
        "dataset = {\n",
        "    'train': CustomDataset(data['train'], tokenizer, max_length=128),\n",
        "    'dev': CustomDataset(data['dev'], tokenizer, max_length=128)\n",
        "}\n",
        "\n",
        "# Initialize data collator for sequence-to-sequence task\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    label_pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Define training arguments with optimized parameters\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=Path('model_checkpoints'),\n",
        "    max_steps=1000,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type='inverse_sqrt',\n",
        "    warmup_steps=2000,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.98,\n",
        "    adam_epsilon=1e-6,\n",
        "    label_smoothing_factor=0.1,\n",
        "\n",
        "    # Evaluation settings\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    eval_delay=200,\n",
        "    metric_for_best_model='exact_match',\n",
        "\n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,  # Keep more checkpoints\n",
        "\n",
        "    # Generation settings\n",
        "    gradient_accumulation_steps=4,\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # Logging\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # Other settings\n",
        "    overwrite_output_dir=True,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['dev'],\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Analysis"
      ],
      "metadata": {
        "id": "46nr2uf0ygVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "MZCTkEBfH__-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eae20285-db18-4ce4-872c-7e42af161cfa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully downloaded 200 lines from eng_200.train\n",
            "Running evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Both `max_new_tokens` (=32) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Prediction Analysis:\n",
            "Analyzing 15 random examples...\n",
            "\n",
            "Detailed Examples:\n",
            "\n",
            "1. ✗ Input: N/A\n",
            "   Prediction: kiled\n",
            "   Target: liked\n",
            "   Edit Distance: 2\n",
            "\n",
            "2. ✗ Input: N/A\n",
            "   Prediction: sticked\n",
            "   Target: stuck\n",
            "   Edit Distance: 3\n",
            "\n",
            "3. ✓ Input: N/A\n",
            "   Prediction: fired\n",
            "   Target: fired\n",
            "   Edit Distance: 0\n",
            "\n",
            "4. ✗ Input: N/A\n",
            "   Prediction: prisured\n",
            "   Target: surprised\n",
            "   Edit Distance: 5\n",
            "\n",
            "5. ✓ Input: N/A\n",
            "   Prediction: crunched\n",
            "   Target: crunched\n",
            "   Edit Distance: 0\n",
            "\n",
            "6. ✗ Input: N/A\n",
            "   Prediction: snaped\n",
            "   Target: snapped\n",
            "   Edit Distance: 1\n",
            "\n",
            "7. ✗ Input: N/A\n",
            "   Prediction: chated\n",
            "   Target: taught\n",
            "   Edit Distance: 6\n",
            "\n",
            "8. ✗ Input: N/A\n",
            "   Prediction: conked\n",
            "   Target: knocked\n",
            "   Edit Distance: 3\n",
            "\n",
            "9. ✗ Input: N/A\n",
            "   Prediction: sted\n",
            "   Target: set\n",
            "   Edit Distance: 2\n",
            "\n",
            "10. ✓ Input: N/A\n",
            "   Prediction: stapled\n",
            "   Target: stapled\n",
            "   Edit Distance: 0\n",
            "\n",
            "11. ✗ Input: N/A\n",
            "   Prediction: ecited\n",
            "   Target: excited\n",
            "   Edit Distance: 1\n",
            "\n",
            "12. ✓ Input: N/A\n",
            "   Prediction: wished\n",
            "   Target: wished\n",
            "   Edit Distance: 0\n",
            "\n",
            "13. ✗ Input: N/A\n",
            "   Prediction: haied\n",
            "   Target: hayed\n",
            "   Edit Distance: 1\n",
            "\n",
            "14. ✓ Input: N/A\n",
            "   Prediction: waved\n",
            "   Target: waved\n",
            "   Edit Distance: 0\n",
            "\n",
            "15. ✗ Input: N/A\n",
            "   Prediction: haved\n",
            "   Target: heaved\n",
            "   Edit Distance: 1\n",
            "\n",
            "Error Statistics:\n",
            "  • total_errors: 295\n",
            "  • length_mismatch: 253\n",
            "\n",
            "📈 Evaluation Results:\n",
            "  • test_loss: 1.0022\n",
            "  • test_exact_match: 0.2550\n",
            "  • test_avg_edit_distance: 1.6667\n",
            "  • test_max_edit_distance: 6.0000\n",
            "  • test_error_rate: 19.6667\n",
            "  • test_runtime: 4.7258\n",
            "  • test_samples_per_second: 42.3210\n",
            "  • test_steps_per_second: 1.4810\n",
            "  • epoch: 125.0000\n",
            "  • test_samples: 200.0000\n",
            "  • eval_time_seconds: 4.7300\n",
            "  • eval_samples_per_second: 42.3100\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, List\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(\n",
        "    trainer,\n",
        "    test_path: str,\n",
        "    download_data,\n",
        "    preprocess_data,\n",
        "    tokenizer,\n",
        "    batch_size: int = 32\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data with detailed metrics.\n",
        "\n",
        "    Args:\n",
        "        trainer: Seq2SeqTrainer instance\n",
        "        test_path: URL to test data\n",
        "        batch_size: Batch size for evaluation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and preprocess test data\n",
        "        print(\"Loading test data...\")\n",
        "        test_data = download_data(test_path)\n",
        "        processed_data = preprocess_data(test_data)\n",
        "        test_dataset = CustomDataset(processed_data, tokenizer)\n",
        "\n",
        "        # Run evaluation\n",
        "        print(\"Running evaluation...\")\n",
        "        start_time = time.time()\n",
        "        result = trainer.evaluate(\n",
        "            test_dataset,\n",
        "            max_length=128,\n",
        "            num_beams=5,\n",
        "            metric_key_prefix=\"test\"\n",
        "        )\n",
        "        eval_time = time.time() - start_time\n",
        "\n",
        "        # Add additional metrics\n",
        "        result.update({\n",
        "            \"test_samples\": len(test_dataset),\n",
        "            \"eval_time_seconds\": round(eval_time, 2),\n",
        "            \"eval_samples_per_second\": round(len(test_dataset) / eval_time, 2)\n",
        "        })\n",
        "\n",
        "        # Print detailed results\n",
        "        print(\"\\n📈 Evaluation Results:\")\n",
        "        for metric, value in result.items():\n",
        "            print(f\"  • {metric}: {value:.4f}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Evaluation failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run evaluation\n",
        "test_data_path = 'https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng_200.train'\n",
        "evaluation_results = evaluate_model(\n",
        "    trainer=trainer,\n",
        "    test_path=test_data_path,\n",
        "    download_data=download_data,\n",
        "    preprocess_data=preprocess_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Store test accuracy\n",
        "test_accuracy = evaluation_results['test_exact_match']\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}