---
title: "ğŸ—“ï¸ Week 05 - Intro to tranformers (part 2)"
---


## ğŸ‘¨â€ğŸ« Lecture Slides

Either click on the slide area below or click [here](slides.qmd) to view it in fullscreen.
Use your keypad to navigate the slides.

<div class="container" style="position: relative;overflow: hidden;width: 100%;min-height:600px;">
<iframe src="slides.html" style="position=absolute;top: 0;left: 0;bottom: 0;right: 0;width: 100%;min-height:600px;"></iframe>
</div>

## âœï¸ Homework


- [ ] **Reading**
	- [ ] Read the first 8 pages of [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper.
	- [ ] Read about Self-attention mechanism here: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
	- [ ] Watch these videos:
		- [ ] [How large language models work, a visual intro to transformers](https://www.youtube.com/watch?v=wjZofJX0v4M)
		- [ ] [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc)
		- [ ] [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)
