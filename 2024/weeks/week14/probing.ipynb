{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probing\n",
        "\n",
        "Probing in large language models (LLMs) for linguistic abilities is used to understand how these models encode and process various aspects of language.\n",
        "\n",
        "There are two main probing paradigms:\n",
        "\n",
        "1. Diagnostic probing: This examines internal neural representations of the model.\n",
        "\n",
        "2. Prompting-based probing: This evaluates model outputs through behavioral tests."
      ],
      "metadata": {
        "id": "xnKFg4zc8IS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Syntactic probing\n",
        "\n",
        "We perform syntactic probing by testing the model's ability to handle subject-verb agreement."
      ],
      "metadata": {
        "id": "myp-12uXmkeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM"
      ],
      "metadata": {
        "id": "uuiaja4Bj_F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sfkfA5sj8nd",
        "outputId": "099eb3d7-9aaa-410b-bd39-d9dc397eac54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We go through all combinations of subject and verb pairs, creating test sentences for both singular and plural forms. We then get the probabilities for each verb form."
      ],
      "metadata": {
        "id": "ttqeR8qNlG92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syntactic_probe(sentence_template, subject_pairs, verb_pairs):\n",
        "    results = {}\n",
        "\n",
        "    for subj_sing, subj_plur in subject_pairs:\n",
        "        for verb_sing, verb_plur in verb_pairs:\n",
        "            sent_sing = sentence_template.format(subject=subj_sing, verb=tokenizer.mask_token)\n",
        "            prob_sing = probe_sentence(sent_sing, [verb_sing, verb_plur])\n",
        "\n",
        "            sent_plur = sentence_template.format(subject=subj_plur, verb=tokenizer.mask_token)\n",
        "            prob_plur = probe_sentence(sent_plur, [verb_sing, verb_plur])\n",
        "\n",
        "            results[(subj_sing, subj_plur, verb_sing, verb_plur)] = {\n",
        "                'singular': prob_sing,\n",
        "                'plural': prob_plur\n",
        "            }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "wVeqMfOpkT2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take a sentence with a masked verb and calculates the probability of each verb option fitting in that mask."
      ],
      "metadata": {
        "id": "PuvAZGQslbXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probe_sentence(sentence, verb_options):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    logits = outputs.logits[0, mask_token_index, :]\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    verb_probs = {}\n",
        "    for verb in verb_options:\n",
        "        verb_id = tokenizer.convert_tokens_to_ids(verb)\n",
        "        verb_probs[verb] = probs[0, verb_id].item()\n",
        "\n",
        "    return verb_probs"
      ],
      "metadata": {
        "id": "4WrCY3XEk-25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide a sentence template, pairs of singular/plural subjects, and pairs of singular/plural verbs."
      ],
      "metadata": {
        "id": "c0Ii2JKslmQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_template = \"The {subject} {verb} in the park.\"\n",
        "subject_pairs = [\n",
        "    (\"dog\", \"dogs\"),\n",
        "    (\"cat\", \"cats\"),\n",
        "    (\"child\", \"children\")\n",
        "]\n",
        "verb_pairs = [\n",
        "    (\"runs\", \"run\"),\n",
        "    (\"plays\", \"play\"),\n",
        "    (\"walks\", \"walk\")\n",
        "]\n",
        "\n",
        "results = syntactic_probe(sentence_template, subject_pairs, verb_pairs)\n",
        "\n",
        "for (subj_sing, subj_plur, verb_sing, verb_plur), probs in results.items():\n",
        "    print(f\"Subject: {subj_sing}/{subj_plur}, Verb: {verb_sing}/{verb_plur}\")\n",
        "    print(f\"Singular agreement:\")\n",
        "    print(f\"  {verb_sing}: {probs['singular'].get(verb_sing, 'N/A'):.4f}\")\n",
        "    print(f\"  {verb_plur}: {probs['singular'].get(verb_plur, 'N/A'):.4f}\")\n",
        "    print(f\"Plural agreement:\")\n",
        "    print(f\"  {verb_sing}: {probs['plural'].get(verb_sing, 'N/A'):.4f}\")\n",
        "    print(f\"  {verb_plur}: {probs['plural'].get(verb_plur, 'N/A'):.4f}\")\n",
        "    print()\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for (subj_sing, subj_plur, verb_sing, verb_plur), probs in results.items():\n",
        "    if verb_sing in probs['singular'] and verb_plur in probs['singular']:\n",
        "        if probs['singular'][verb_sing] > probs['singular'][verb_plur]:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "    if verb_sing in probs['plural'] and verb_plur in probs['plural']:\n",
        "        if probs['plural'][verb_plur] > probs['plural'][verb_sing]:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "print(f\"Overall accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFJqfQcFlldB",
        "outputId": "1a523a34-df0b-469d-9e85-64be39940f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: dog/dogs, Verb: runs/run\n",
            "Singular agreement:\n",
            "  runs: 0.0102\n",
            "  run: 0.0001\n",
            "Plural agreement:\n",
            "  runs: 0.0001\n",
            "  run: 0.0081\n",
            "\n",
            "Subject: dog/dogs, Verb: plays/play\n",
            "Singular agreement:\n",
            "  plays: 0.0034\n",
            "  play: 0.0001\n",
            "Plural agreement:\n",
            "  plays: 0.0001\n",
            "  play: 0.0279\n",
            "\n",
            "Subject: dog/dogs, Verb: walks/walk\n",
            "Singular agreement:\n",
            "  walks: 0.0165\n",
            "  walk: 0.0003\n",
            "Plural agreement:\n",
            "  walks: 0.0001\n",
            "  walk: 0.0076\n",
            "\n",
            "Subject: cat/cats, Verb: runs/run\n",
            "Singular agreement:\n",
            "  runs: 0.0086\n",
            "  run: 0.0002\n",
            "Plural agreement:\n",
            "  runs: 0.0001\n",
            "  run: 0.0057\n",
            "\n",
            "Subject: cat/cats, Verb: plays/play\n",
            "Singular agreement:\n",
            "  plays: 0.0083\n",
            "  play: 0.0004\n",
            "Plural agreement:\n",
            "  plays: 0.0002\n",
            "  play: 0.0839\n",
            "\n",
            "Subject: cat/cats, Verb: walks/walk\n",
            "Singular agreement:\n",
            "  walks: 0.0260\n",
            "  walk: 0.0008\n",
            "Plural agreement:\n",
            "  walks: 0.0002\n",
            "  walk: 0.0102\n",
            "\n",
            "Subject: child/children, Verb: runs/run\n",
            "Singular agreement:\n",
            "  runs: 0.0083\n",
            "  run: 0.0002\n",
            "Plural agreement:\n",
            "  runs: 0.0000\n",
            "  run: 0.0024\n",
            "\n",
            "Subject: child/children, Verb: plays/play\n",
            "Singular agreement:\n",
            "  plays: 0.0202\n",
            "  play: 0.0007\n",
            "Plural agreement:\n",
            "  plays: 0.0008\n",
            "  play: 0.3045\n",
            "\n",
            "Subject: child/children, Verb: walks/walk\n",
            "Singular agreement:\n",
            "  walks: 0.0213\n",
            "  walk: 0.0006\n",
            "Plural agreement:\n",
            "  walks: 0.0002\n",
            "  walk: 0.0079\n",
            "\n",
            "Overall accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pragmatics probing\n",
        "\n",
        "Pragmatics probing tests whether an LLM can infer meanings beyond literal interpretations by considering context, social norms, and cultural factors."
      ],
      "metadata": {
        "id": "_KcHXg-C4gjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def probe_pragmatics(context, question, options):\n",
        "    input_text = f\"Context: {context}\\n\\nQuestion: {question}\\nOptions: {', '.join(options)}\\n\\nAnswer:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=20, num_return_sequences=len(options), num_beams=3)\n",
        "\n",
        "    decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "    option_probs = {}\n",
        "    for option, output in zip(options, decoded_outputs):\n",
        "        option_probs[option] = 1 if option.lower() in output.lower() else 0\n",
        "\n",
        "    total = sum(option_probs.values())\n",
        "    if total > 0:\n",
        "        option_probs = {k: v / total for k, v in option_probs.items()}\n",
        "    else:\n",
        "        option_probs = {k: 1 / len(options) for k in options}\n",
        "\n",
        "    return option_probs\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"context\": \"Alice: Did you enjoy the movie?\\nBob: Well, the popcorn was good.\",\n",
        "        \"question\": \"What does Bob mean?\",\n",
        "        \"options\": [\n",
        "            \"He liked the movie.\",\n",
        "            \"He did not enjoy the movie.\",\n",
        "            \"He only enjoyed the popcorn.\"\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Teacher: Can you explain why you missed class yesterday?\\nStudent: I had a family emergency.\",\n",
        "        \"question\": \"What is the student implying?\",\n",
        "        \"options\": [\n",
        "            \"The student wants sympathy.\",\n",
        "            \"The student is making an excuse.\",\n",
        "            \"The student is avoiding answering.\"\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Friend 1: Do you want to go out tonight?\\nFriend 2: I have a lot of work to do.\",\n",
        "        \"question\": \"What does Friend 2 mean?\",\n",
        "        \"options\": [\n",
        "            \"They want to go out later.\",\n",
        "            \"They are politely declining.\",\n",
        "            \"They are asking for help with work.\"\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    print(f\"Context:\\n{example['context']}\")\n",
        "    print(f\"Question: {example['question']}\")\n",
        "\n",
        "    results = probe_pragmatics(\n",
        "        example[\"context\"],\n",
        "        example[\"question\"],\n",
        "        example[\"options\"]\n",
        "    )\n",
        "\n",
        "    print(\"Probabilities for each option:\")\n",
        "    for option, prob in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {option}: {prob:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prpVCfLxmLu-",
        "outputId": "01613bad-89f6-470a-98e1-d87408f681d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "Alice: Did you enjoy the movie?\n",
            "Bob: Well, the popcorn was good.\n",
            "Question: What does Bob mean?\n",
            "Probabilities for each option:\n",
            "  He did not enjoy the movie.: 1.0000\n",
            "  He liked the movie.: 0.0000\n",
            "  He only enjoyed the popcorn.: 0.0000\n",
            "\n",
            "==================================================\n",
            "\n",
            "Context:\n",
            "Teacher: Can you explain why you missed class yesterday?\n",
            "Student: I had a family emergency.\n",
            "Question: What is the student implying?\n",
            "Probabilities for each option:\n",
            "  The student is avoiding answering.: 1.0000\n",
            "  The student wants sympathy.: 0.0000\n",
            "  The student is making an excuse.: 0.0000\n",
            "\n",
            "==================================================\n",
            "\n",
            "Context:\n",
            "Friend 1: Do you want to go out tonight?\n",
            "Friend 2: I have a lot of work to do.\n",
            "Question: What does Friend 2 mean?\n",
            "Probabilities for each option:\n",
            "  They want to go out later.: 0.3333\n",
            "  They are politely declining.: 0.3333\n",
            "  They are asking for help with work.: 0.3333\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8C5Gn-s41RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}