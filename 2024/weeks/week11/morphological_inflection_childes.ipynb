{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Morphological inflection with T5 models using CHILDES datasets"
      ],
      "metadata": {
        "id": "Bv4ehYRRvR4K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_kmKAi-nMc"
      },
      "source": [
        "- You can either train a t5 model from the scratch or finetune an existing t5 model for the morphological inflection task. You need to try to beat the baseline accuracies mentioned below each language.\n",
        "\n",
        "- You can choose the language of your choice. We have three datasets for 3 languages.\n",
        "\n",
        "- The datasets are taken from the SIGMORPHON shared tasks (https://github.com/sigmorphon/2022InflectionST/tree/main/part2). The training sets are sampled weighted by frequency from German and English child-directed speech corpora available from UniMorph with frequencies from the CHILDES database [MacWhinney 2000](https://childes.talkbank.org/access/) such that the smallest training sets contain only the highest frequency words. Arabic is sampled in the same way, but words and their frequencies are taken from the [Penn Arabic Treebank](https://www.marefa.org/images/e/e8/The_penn_arabic_treebank_Building_a_large-scale_an_%281%29.pdf)\n",
        "\n",
        "- You need to write a summary consisting of:\n",
        "  - Data description\n",
        "  - About the problem (e.g., English Tense Debate)\n",
        "  - Changes you've made to the model\n",
        "  - Explain the architecture of the model\n",
        "  - Hypothesize why those changes might have improved the accuracy\n",
        "\n",
        "\n",
        "### English:\n",
        "\n",
        "English Past-Tense Debate (e.g., [Marcus et al. 1992](https://www.jstor.org/stable/1166115?seq=1#metadata_info_tab_contents))\n",
        "\n",
        "Training data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng_1000.train\n",
        "\n",
        "Development data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng.dev\n",
        "\n",
        "Test data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/eng.gold\n",
        "\n",
        "Basline accuracy: 65%\n",
        "\n",
        "### German:\n",
        "\n",
        "German noun plurals, a well-studied challenge case which may have a minority-default pattern ([Clahsen et al. 1992](https://www.sciencedirect.com/science/article/pii/001002779290018D), [Marcus et al. 1995](https://d1wqtxts1xzle7.cloudfront.net/30270110/Marcus_Pinker_et_al_1995_German_Inflection_Cognitive_Psychology-with-cover-page-v2.pdf?Expires=1646265841&Signature=PRNt6JeRUZYQ0KBtfJMzRH3cQPySiWtycYIZqkYPBoxn2-Y3k6zgLMpUHKLE3RFPMajxCT0ReU-~CuADL66-hk7zI9eT6pcoj-jBOTr5Yt4NbjEoHs~o4-AXB6J1sdKcKLqMLH3x6h41Dtnp-tgviym3GV42e6usK0yQyMM9O0KiEY~nWulXAqVFTeY~CL8~0PBYEHXRywsTm6ZOMI7kTZzefyg1ZLGlrGtHcZyHMV4KO0ibT7SddhQgiiuHh6j4jIlCwdxiovf~MPqu5lpJqxDdlOoJS8AktpmsCTipAw4Q2~frNXr1rJ2GM2WBUABjugH0JbBhhvB4TpLzPZ6qrA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA))\n",
        "\n",
        "Training data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu_600.train\n",
        "\n",
        "Development data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu.dev\n",
        "\n",
        "Test data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu.gold\n",
        "\n",
        "Baseline accuracy: 55%\n",
        "\n",
        "### Arabic:\n",
        "\n",
        "Arabic noun plurals, with several competing affixal and templatic patterns ([Ravid & Farah 1999](https://journals.sagepub.com/doi/pdf/10.1177/014272379901905603?casa_token=RHoIAWxOousAAAAA:NpjamGN3dzbA43WuEpZzKbBApqyYol5jI9vqJ3C7NKGigY5nSmm5ZA18sciRfWFESETqXL21chgi), [Dawdy-Hesterberg & Pierrehumbert (2014)](https://www.tandfonline.com/doi/pdf/10.1080/23273798.2014.899377)).\n",
        "\n",
        "Training data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/ara_900.train\n",
        "\n",
        "Development data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/ara.dev\n",
        "\n",
        "Test data: https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/ara.gold\n",
        "\n",
        "Baseline accuracy: 43%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBtFvjGFCGx8"
      },
      "source": [
        "## Training T5 model from the scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGoyOLDLCC0s"
      },
      "outputs": [],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U evaluate\n",
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojVbecrfCfrR"
      },
      "outputs": [],
      "source": [
        "## Copy-paste the training, development and test data based on what task you've chosen\n",
        "\n",
        "train_data_path = \"https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu_600.train\" # fill me\n",
        "dev_data_path = \"https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu.dev\" # fill me\n",
        "test_data_path = \"https://raw.githubusercontent.com/sigmorphon/2022InflectionST/refs/heads/main/part2/deu.gold\" # fill me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWGZbx0lCZhy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from typing import List, Dict\n",
        "from requests.exceptions import RequestException\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "\n",
        "def download_data(url: str, max_retries: int = 3, timeout: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Download and process data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to download data from\n",
        "        max_retries: Maximum number of retry attempts\n",
        "        timeout: Timeout in seconds for the request\n",
        "\n",
        "    Returns:\n",
        "        List of strings, each representing a line from the downloaded data\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            data = response.text.strip().split('\\n')\n",
        "            print(f\"‚úì Successfully downloaded {len(data)} lines from {url.split('/')[-1]}\")\n",
        "            return data\n",
        "\n",
        "        except RequestException as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                display(HTML(f\"<b style='color: red'>Error downloading {url.split('/')[-1]}: {str(e)}</b>\"))\n",
        "                raise\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed, retrying...\")\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "# Download and store data\n",
        "data = {}\n",
        "try:\n",
        "    data['train'] = download_data(train_data_path)\n",
        "    data['dev'] = download_data(dev_data_path)\n",
        "except Exception as e:\n",
        "    display(HTML(f\"<b style='color: red'>Failed to load datasets: {str(e)}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX6AAOMcC2HH"
      },
      "outputs": [],
      "source": [
        "# Display dataset statistics with detailed formatting\n",
        "def display_data_summary(data: dict) -> None:\n",
        "    \"\"\"\n",
        "    Display summary statistics and sample data in a formatted way.\n",
        "\n",
        "    Args:\n",
        "        data: Dictionary containing 'train' and 'dev' datasets\n",
        "    \"\"\"\n",
        "    # Print dataset sizes with formatted output\n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"  ‚Ä¢ Training samples: {len(data['train']):,}\")\n",
        "    print(f\"  ‚Ä¢ Development samples: {len(data['dev']):,}\")\n",
        "\n",
        "    sample = data['train'][0]\n",
        "    print(f\"\\nüìù Sample Data Format:\", sample)\n",
        "\n",
        "# Display the summary\n",
        "display_data_summary(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9WOb-wlC6Vz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import regex\n",
        "from typing import List, Tuple, Set, Dict\n",
        "\n",
        "def parse_tag(tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Parse a tag string by removing unwanted characters and formatting each tag.\n",
        "\n",
        "    Args:\n",
        "        tag: Input tag string containing tags separated by delimiters\n",
        "\n",
        "    Returns:\n",
        "        Formatted string with each tag wrapped in angle brackets\n",
        "    \"\"\"\n",
        "    tag = re.sub(r\"\\)|\\(|,|;\", ' ', tag).split()\n",
        "    return ''.join(['<{}>'.format(t) for t in tag])\n",
        "\n",
        "\n",
        "def preprocess_data(raw_data: List[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Preprocess raw data by parsing each line into formatted input-target pairs.\n",
        "\n",
        "    Args:\n",
        "        raw_data: List of raw data lines (lemma, target, tag separated by tabs)\n",
        "\n",
        "    Returns:\n",
        "        List of tuples containing (formatted_input, target)\n",
        "    \"\"\"\n",
        "    preprocessed_data = []\n",
        "    for line in raw_data:\n",
        "        try:\n",
        "            lemma, target, tag = line.split('\\t')\n",
        "            formatted_input = f\"{lemma} {parse_tag(tag)}\"\n",
        "            preprocessed_data.append((formatted_input, target))\n",
        "        except ValueError as e:\n",
        "            print(f\"‚ö†Ô∏è Skipping malformed line: {line} (Error: {e})\")\n",
        "\n",
        "    return preprocessed_data\n",
        "\n",
        "def extract_vocab(data: List[Tuple[str, str]]) -> Tuple[Set[str], List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Extract vocabulary, tags, and character mappings from preprocessed data.\n",
        "\n",
        "    Args:\n",
        "        data: List of preprocessed (input, target) pairs\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Set of unique characters\n",
        "        - List of unique tags\n",
        "        - Dictionary mapping characters to indices\n",
        "    \"\"\"\n",
        "    # Extract characters from lemmas and targets\n",
        "    chars = set(''.join([d[0].split()[0] + d[1] for d in data]))\n",
        "\n",
        "    # Create character to index mapping\n",
        "    char2id = {char: idx for idx, char in enumerate(sorted(chars))}\n",
        "\n",
        "    # Extract unique tags\n",
        "    tags = list(set(regex.findall(r\"<[A-Za-z0-9]+>\", ' '.join(d[0] for d in data))))\n",
        "\n",
        "    return chars, tags, char2id\n",
        "\n",
        "# Process the data\n",
        "data['train'] = preprocess_data(data['train'])\n",
        "data['dev'] = preprocess_data(data['dev'])\n",
        "\n",
        "# Display sample of preprocessed data\n",
        "print(f\"\\nüìù Preprocessed sample:\")\n",
        "print(f\"  {data['train'][0]}\")\n",
        "\n",
        "# Extract vocabulary and mappings\n",
        "chars, tags, char2id = extract_vocab(data['train'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Nh1zgfC8E3"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, List, Union\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "@dataclass\n",
        "class TokenizerConfig:\n",
        "    \"\"\"Configuration for CustomTokenizer\"\"\"\n",
        "    bos_token: str = \"<s>\"\n",
        "    eos_token: str = \"</s>\"\n",
        "    unk_token: str = \"<unk>\"\n",
        "    pad_token: str = \"<pad>\"\n",
        "    max_len: int = 512\n",
        "\n",
        "class CustomTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    Custom tokenizer for character-level tokenization with special handling for morphosyntactic features.\n",
        "\n",
        "    Processes input and output word forms character by character while treating morphosyntactic\n",
        "    features as special atomic tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Dict[str, int],\n",
        "        additional_special_tokens: Optional[List[str]] = None,\n",
        "        max_len: int = 512,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocab: Dictionary mapping characters to their IDs\n",
        "            additional_special_tokens: List of special tokens (e.g., morphological tags)\n",
        "            max_len: Maximum sequence length\n",
        "        \"\"\"\n",
        "        config = TokenizerConfig()\n",
        "\n",
        "        # Initialize token mappings\n",
        "        self.__token_ids = vocab.copy()\n",
        "        self.__id_tokens = {v: k for k, v in vocab.items()}\n",
        "\n",
        "        # Process special tokens\n",
        "        special_tokens = {\n",
        "            'pad_token': self._create_added_token(config.pad_token),\n",
        "            'bos_token': self._create_added_token(config.bos_token),\n",
        "            'eos_token': self._create_added_token(config.eos_token),\n",
        "            'unk_token': self._create_added_token(config.unk_token)\n",
        "        }\n",
        "\n",
        "        # Initialize special token decoder\n",
        "        self._added_tokens_decoder = {\n",
        "            i: token for i, token in enumerate(special_tokens.values())\n",
        "        }\n",
        "        self.offset = len(self._added_tokens_decoder)\n",
        "\n",
        "        super().__init__(\n",
        "            **special_tokens,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            max_len=max_len,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_added_token(token: Union[str, AddedToken]) -> AddedToken:\n",
        "        \"\"\"Create an AddedToken with consistent settings.\"\"\"\n",
        "        return AddedToken(token, lstrip=False, rstrip=False) if isinstance(token, str) else token\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Get the size of the vocabulary.\"\"\"\n",
        "        return len(self.__token_ids)\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        \"\"\"Get the full vocabulary including special tokens.\"\"\"\n",
        "        vocab = {\n",
        "            self.convert_ids_to_tokens(i): i\n",
        "            for i in range(self.vocab_size + self.offset)\n",
        "        }\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _add_eos(self, token_ids: List[int]) -> List[int]:\n",
        "        \"\"\"Add end-of-sequence token to the token list.\"\"\"\n",
        "        return token_ids + [self.eos_token_id]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Create token type IDs for single or paired sequences.\"\"\"\n",
        "        total_length = len(token_ids_0) + 1  # +1 for EOS\n",
        "        if token_ids_1:\n",
        "            total_length += len(token_ids_1) + 1\n",
        "        return [0] * total_length\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Build model inputs by adding special tokens.\"\"\"\n",
        "        token_ids_0 = self._add_eos(token_ids_0)\n",
        "        if token_ids_1 is None:\n",
        "            return token_ids_0\n",
        "        return token_ids_0 + self._add_eos(token_ids_1)\n",
        "\n",
        "    def _tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        \"\"\"Tokenize text into characters.\"\"\"\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token to its ID.\"\"\"\n",
        "        return (self.__token_ids.get(token, self.unk_token_id - self.offset)\n",
        "                + self.offset)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        \"\"\"Convert an ID back to its token.\"\"\"\n",
        "        adjusted_index = index - self.offset\n",
        "        return self.__id_tokens.get(adjusted_index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Convert tokens back to string.\"\"\"\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocabulary(self,\n",
        "                       save_directory: str,\n",
        "                       filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        \"\"\"Save the vocabulary to a file.\"\"\"\n",
        "        prefix = filename_prefix or \"\"\n",
        "        vocab_path = Path(save_directory) / f\"{prefix}vocab.json\"\n",
        "\n",
        "        with vocab_path.open('w', encoding='utf-8') as f:\n",
        "            json.dump(self.__token_ids, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return (str(vocab_path),)\n",
        "\n",
        "# Initialize tokenizer with vocabulary and tags\n",
        "tokenizer = CustomTokenizer(char2id, additional_special_tokens=tags, max_len=100)\n",
        "\n",
        "# Display example tokenization\n",
        "sample_idx = 54\n",
        "sample_text = data['train'][sample_idx][0]\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(f\"Tokenization Example:\")\n",
        "print(f\"Input text: {sample_text}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4dBeUczC-Mf"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Config, T5ForConditionalGeneration\n",
        "from typing import Optional\n",
        "\n",
        "def create_t5_model(\n",
        "    tokenizer,\n",
        "    d_ff: int = 1024,\n",
        "    d_model: int = 256,\n",
        "    num_layers: int = 4,\n",
        "    num_heads: int = 4,\n",
        "    dropout_rate: float = 0.2,\n",
        "    max_new_tokens: int = 32,\n",
        "    d_kv: Optional[int] = None\n",
        ") -> T5ForConditionalGeneration:\n",
        "    \"\"\"\n",
        "    Creates and configures a T5 model for morphological generation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Custom tokenizer for the model\n",
        "        d_ff: Dimension of the feed-forward layer\n",
        "        d_model: Dimension of the model embeddings\n",
        "        num_layers: Number of encoder and decoder layers\n",
        "        num_heads: Number of attention heads\n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        d_kv: Dimension of key and value vectors (defaults to d_model // num_heads)\n",
        "\n",
        "    Returns:\n",
        "        Configured T5ForConditionalGeneration model\n",
        "    \"\"\"\n",
        "    # Calculate default d_kv if not provided\n",
        "    if d_kv is None:\n",
        "        d_kv = d_model // num_heads\n",
        "\n",
        "    # Configure model architecture\n",
        "    config = T5Config(\n",
        "        d_ff=d_ff,\n",
        "        d_model=d_model,\n",
        "        d_kv=d_kv,\n",
        "        num_layers=num_layers,\n",
        "        num_decoder_layers=num_layers,  # Match encoder layers\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate,\n",
        "        vocab_size=len(tokenizer),\n",
        "        is_encoder_decoder=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = T5ForConditionalGeneration(config)\n",
        "\n",
        "    # Configure generation parameters\n",
        "    model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "    model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "    model.generation_config.max_new_tokens = max_new_tokens\n",
        "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.generation_config.no_repeat_ngram_size = 2\n",
        "    model.generation_config.length_penalty = 1.0\n",
        "    model.generation_config.num_beams = 10\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the model with default parameters\n",
        "model = create_t5_model(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZOdr6vPDAMZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from typing import List, Dict, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for morphological generation task.\n",
        "    Handles tokenization of input lemmas and target forms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: List[Tuple[str, str]],\n",
        "        tokenizer,\n",
        "        max_length: int = 128\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "\n",
        "        Args:\n",
        "            data: List of (input, target) string pairs\n",
        "            tokenizer: Tokenizer for encoding text\n",
        "            max_length: Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return dataset size.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get tokenized item from dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with input_ids and labels tensors\n",
        "        \"\"\"\n",
        "        input_text, target_text = self.data[idx]\n",
        "\n",
        "        # Tokenize input and target\n",
        "        model_inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = self.tokenizer(\n",
        "            target_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": model_inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "def postprocess_data(\n",
        "    token_ids: Union[np.ndarray, torch.Tensor],\n",
        "    tokenizer\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Post-process token IDs into readable text.\n",
        "\n",
        "    Args:\n",
        "        token_ids: Array/Tensor of token IDs\n",
        "        tokenizer: Tokenizer for decoding\n",
        "\n",
        "    Returns:\n",
        "        List of decoded strings with special tokens removed\n",
        "    \"\"\"\n",
        "    # Convert torch tensor to numpy if needed\n",
        "    if isinstance(token_ids, torch.Tensor):\n",
        "        token_ids = token_ids.cpu().numpy()\n",
        "\n",
        "    # Replace padding indices\n",
        "    token_ids = np.where(token_ids != -100, token_ids, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode tokens to strings\n",
        "    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0MNMXS8DCOM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Evaluation metrics for morphological generation task.\n",
        "Includes exact match scoring and detailed error analysis.\n",
        "\"\"\"\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import Levenshtein\n",
        "from collections import defaultdict\n",
        "\n",
        "@dataclass\n",
        "class PredictionExample:\n",
        "    \"\"\"Store prediction examples for analysis\"\"\"\n",
        "    input: str\n",
        "    prediction: str\n",
        "    target: str\n",
        "    is_correct: bool\n",
        "    edit_distance: int\n",
        "\n",
        "class MetricsComputer:\n",
        "    \"\"\"Compute and analyze model predictions\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, sample_size: int = 15):\n",
        "        \"\"\"\n",
        "        Initialize metrics computer.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: Tokenizer for decoding predictions\n",
        "            sample_size: Number of random examples to sample for analysis\n",
        "        \"\"\"\n",
        "        self.metric = evaluate.load(\"exact_match\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_size = sample_size\n",
        "        self.error_stats = defaultdict(int)\n",
        "\n",
        "    def compute_edit_distance(self, pred: str, target: str) -> int:\n",
        "        \"\"\"Compute Levenshtein distance between prediction and target\"\"\"\n",
        "        return Levenshtein.distance(pred, target)\n",
        "\n",
        "    def analyze_predictions(\n",
        "        self,\n",
        "        decoded_preds: List[str],\n",
        "        decoded_labels: List[str],\n",
        "        inputs: List[str] = None\n",
        "    ) -> List[PredictionExample]:\n",
        "        \"\"\"\n",
        "        Analyze a sample of predictions.\n",
        "\n",
        "        Args:\n",
        "            decoded_preds: List of model predictions\n",
        "            decoded_labels: List of target labels\n",
        "            inputs: Optional list of input texts\n",
        "\n",
        "        Returns:\n",
        "            List of PredictionExample objects\n",
        "        \"\"\"\n",
        "        sample_indices = random.sample(range(len(decoded_preds)),\n",
        "                                     min(self.sample_size, len(decoded_preds)))\n",
        "\n",
        "        examples = []\n",
        "        for idx in sample_indices:\n",
        "            pred = decoded_preds[idx]\n",
        "            target = decoded_labels[idx]\n",
        "            input_text = inputs[idx] if inputs else \"\"\n",
        "\n",
        "            example = PredictionExample(\n",
        "                input=input_text,\n",
        "                prediction=pred,\n",
        "                target=target,\n",
        "                is_correct=pred == target,\n",
        "                edit_distance=self.compute_edit_distance(pred, target)\n",
        "            )\n",
        "            examples.append(example)\n",
        "\n",
        "            # Track error types\n",
        "            if not example.is_correct:\n",
        "                self.error_stats['total_errors'] += 1\n",
        "                if len(pred) != len(target):\n",
        "                    self.error_stats['length_mismatch'] += 1\n",
        "                if pred == target[::-1]:\n",
        "                    self.error_stats['reversed'] += 1\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def compute_metrics(self, eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute evaluation metrics for predictions.\n",
        "\n",
        "        Args:\n",
        "            eval_preds: Tuple of (predictions, labels) arrays\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of metric scores\n",
        "        \"\"\"\n",
        "        preds, labels = eval_preds\n",
        "        decoded_preds = postprocess_data(preds, self.tokenizer)\n",
        "        decoded_labels = postprocess_data(labels, self.tokenizer)\n",
        "\n",
        "        # Compute exact match score\n",
        "        exact_match = self.metric.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels\n",
        "        )\n",
        "\n",
        "        # Analyze sample predictions\n",
        "        examples = self.analyze_predictions(decoded_preds, decoded_labels)\n",
        "\n",
        "        # Compute additional metrics\n",
        "        edit_distances = [ex.edit_distance for ex in examples]\n",
        "\n",
        "        metrics = {\n",
        "            \"accuracy\": exact_match[\"exact_match\"],\n",
        "        }\n",
        "\n",
        "        # Print analysis examples\n",
        "        self._print_analysis(examples)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _print_analysis(self, examples: List[PredictionExample]) -> None:\n",
        "        \"\"\"Print detailed analysis of prediction examples\"\"\"\n",
        "        print(\"\\nüìä Prediction Analysis:\")\n",
        "        print(f\"Analyzing {len(examples)} random examples...\")\n",
        "        print(\"\\nDetailed Examples:\")\n",
        "\n",
        "        for i, ex in enumerate(examples, 1):\n",
        "            status = \"‚úì\" if ex.is_correct else \"‚úó\"\n",
        "            print(f\"\\n{i}. {status} Input: {ex.input}\")\n",
        "            print(f\"   Prediction: {ex.prediction}\")\n",
        "            print(f\"   Target: {ex.target}\")\n",
        "            print(f\"   Edit Distance: {ex.edit_distance}\")\n",
        "\n",
        "        print(\"\\nError Statistics:\")\n",
        "        for error_type, count in self.error_stats.items():\n",
        "            print(f\"  ‚Ä¢ {error_type}: {count}\")\n",
        "\n",
        "# Initialize metrics computer\n",
        "metrics_computer = MetricsComputer(tokenizer)\n",
        "\n",
        "# Use for evaluation\n",
        "compute_metrics = metrics_computer.compute_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srns_FRoDETl"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from transformers import utils\n",
        "\n",
        "# Monkey patch the deprecated warning in transformers utils\n",
        "utils.deprecation_warning = lambda *args, **kwargs: None\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from pathlib import Path\n",
        "\n",
        "# Create datasets with proper batch size\n",
        "dataset = {\n",
        "    'train': CustomDataset(data['train'], tokenizer, max_length=128),\n",
        "    'dev': CustomDataset(data['dev'], tokenizer, max_length=128)\n",
        "}\n",
        "\n",
        "# Initialize data collator for sequence-to-sequence task\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    label_pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Define training arguments with optimized parameters\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=Path('model_checkpoints'),\n",
        "    max_steps=2000,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type='inverse_sqrt',\n",
        "    warmup_steps=2000,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.98,\n",
        "    adam_epsilon=1e-6,\n",
        "    label_smoothing_factor=0.1,\n",
        "\n",
        "    # Evaluation settings\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    eval_delay=200,\n",
        "    metric_for_best_model='exact_match',\n",
        "\n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=5000,\n",
        "    save_total_limit=3,  # Keep more checkpoints\n",
        "\n",
        "    # Generation settings\n",
        "    gradient_accumulation_steps=4,\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # Logging\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # Other settings\n",
        "    overwrite_output_dir=True,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['dev'],\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk0nUHhSDG9H"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(\n",
        "    trainer,\n",
        "    test_path: str,\n",
        "    download_data,\n",
        "    preprocess_data,\n",
        "    tokenizer,\n",
        "    batch_size: int = 32\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data with detailed metrics.\n",
        "\n",
        "    Args:\n",
        "        trainer: Seq2SeqTrainer instance\n",
        "        test_path: URL to test data\n",
        "        batch_size: Batch size for evaluation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and preprocess test data\n",
        "        print(\"Loading test data...\")\n",
        "        test_data = download_data(test_path)\n",
        "        processed_data = preprocess_data(test_data)\n",
        "        test_dataset = CustomDataset(processed_data, tokenizer)\n",
        "\n",
        "        # Run evaluation\n",
        "        print(\"Running evaluation...\")\n",
        "        start_time = time.time()\n",
        "        result = trainer.evaluate(\n",
        "            test_dataset,\n",
        "            max_length=128,\n",
        "            num_beams=5,\n",
        "            metric_key_prefix=\"test\"\n",
        "        )\n",
        "        eval_time = time.time() - start_time\n",
        "\n",
        "        # Add additional metrics\n",
        "        result.update({\n",
        "            \"test_samples\": len(test_dataset),\n",
        "            \"eval_time_seconds\": round(eval_time, 2),\n",
        "            \"eval_samples_per_second\": round(len(test_dataset) / eval_time, 2)\n",
        "        })\n",
        "\n",
        "        # Print detailed results\n",
        "        print(\"\\nüìà Evaluation Results:\")\n",
        "        for metric, value in result.items():\n",
        "            print(f\"  ‚Ä¢ {metric}: {value:.4f}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_model(\n",
        "    trainer=trainer,\n",
        "    test_path=test_data_path,\n",
        "    download_data=download_data,\n",
        "    preprocess_data=preprocess_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Store test accuracy\n",
        "test_accuracy = evaluation_results['test_exact_match']\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning T5 model"
      ],
      "metadata": {
        "id": "Z1Pzm--ysO0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,          # For automatic tokenizer loading based on model name\n",
        "    DataCollatorForSeq2Seq, # Handles batching and padding for sequence-to-sequence tasks\n",
        "    AutoModelForSeq2SeqLM,  # For automatic loading of sequence-to-sequence models\n",
        "    Seq2SeqTrainingArguments, # Contains training configuration\n",
        "    Seq2SeqTrainer,         # Handles the training loop for sequence-to-sequence models\n",
        "    pipeline               # Provides easy-to-use interfaces for various NLP tasks\n",
        ")\n"
      ],
      "metadata": {
        "id": "7fwQGV3isOQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from typing import List, Dict\n",
        "from requests.exceptions import RequestException\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "\n",
        "def download_data(url: str, max_retries: int = 3, timeout: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Download and process data from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to download data from\n",
        "        max_retries: Maximum number of retry attempts\n",
        "        timeout: Timeout in seconds for the request\n",
        "\n",
        "    Returns:\n",
        "        List of strings, each representing a line from the downloaded data\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            data = response.text.strip().split('\\n')\n",
        "            print(f\"‚úì Successfully downloaded {len(data)} lines from {url.split('/')[-1]}\")\n",
        "            return data\n",
        "\n",
        "        except RequestException as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                display(HTML(f\"<b style='color: red'>Error downloading {url.split('/')[-1]}: {str(e)}</b>\"))\n",
        "                raise\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed, retrying...\")\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "# Download and store data\n",
        "data = {}\n",
        "try:\n",
        "    data['train'] = download_data(train_data_path)\n",
        "    data['dev'] = download_data(dev_data_path)\n",
        "except Exception as e:\n",
        "    display(HTML(f\"<b style='color: red'>Failed to load datasets: {str(e)}</b>\"))"
      ],
      "metadata": {
        "id": "9YBfb20yt7Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset statistics with detailed formatting\n",
        "def display_data_summary(data: dict) -> None:\n",
        "    \"\"\"\n",
        "    Display summary statistics and sample data in a formatted way.\n",
        "\n",
        "    Args:\n",
        "        data: Dictionary containing 'train' and 'dev' datasets\n",
        "    \"\"\"\n",
        "    # Print dataset sizes with formatted output\n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"  ‚Ä¢ Training samples: {len(data['train']):,}\")\n",
        "    print(f\"  ‚Ä¢ Development samples: {len(data['dev']):,}\")\n",
        "\n",
        "    sample = data['train'][0]\n",
        "    print(f\"\\nüìù Sample Data Format:\", sample)\n",
        "\n",
        "# Display the summary\n",
        "display_data_summary(data)\n"
      ],
      "metadata": {
        "id": "r03EJJ6Ut8vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import regex\n",
        "from typing import List, Tuple, Set, Dict\n",
        "\n",
        "def parse_tag(tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Parse a tag string by removing unwanted characters and formatting each tag.\n",
        "\n",
        "    Args:\n",
        "        tag: Input tag string containing tags separated by delimiters\n",
        "\n",
        "    Returns:\n",
        "        Formatted string with each tag wrapped in angle brackets\n",
        "    \"\"\"\n",
        "    tag = re.sub(r\"\\)|\\(|,|;\", ' ', tag).split()\n",
        "    return ''.join(['<{}>'.format(t) for t in tag])\n",
        "\n",
        "\n",
        "def preprocess_data(raw_data: List[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Preprocess raw data by parsing each line into formatted input-target pairs.\n",
        "\n",
        "    Args:\n",
        "        raw_data: List of raw data lines (lemma, target, tag separated by tabs)\n",
        "\n",
        "    Returns:\n",
        "        List of tuples containing (formatted_input, target)\n",
        "    \"\"\"\n",
        "    preprocessed_data = []\n",
        "    for line in raw_data:\n",
        "        try:\n",
        "            lemma, target, tag = line.split('\\t')\n",
        "            formatted_input = f\"{lemma} {parse_tag(tag)}\"\n",
        "            preprocessed_data.append((formatted_input, target))\n",
        "        except ValueError as e:\n",
        "            print(f\"‚ö†Ô∏è Skipping malformed line: {line} (Error: {e})\")\n",
        "\n",
        "    return preprocessed_data\n",
        "\n",
        "def extract_vocab(data: List[Tuple[str, str]]) -> Tuple[Set[str], List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Extract vocabulary, tags, and character mappings from preprocessed data.\n",
        "\n",
        "    Args:\n",
        "        data: List of preprocessed (input, target) pairs\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Set of unique characters\n",
        "        - List of unique tags\n",
        "        - Dictionary mapping characters to indices\n",
        "    \"\"\"\n",
        "    # Extract characters from lemmas and targets\n",
        "    chars = set(''.join([d[0].split()[0] + d[1] for d in data]))\n",
        "\n",
        "    # Create character to index mapping\n",
        "    char2id = {char: idx for idx, char in enumerate(sorted(chars))}\n",
        "\n",
        "    # Extract unique tags\n",
        "    tags = list(set(regex.findall(r\"<[A-Za-z0-9]+>\", ' '.join(d[0] for d in data))))\n",
        "\n",
        "    return chars, tags, char2id\n",
        "\n",
        "# Process the data\n",
        "data['train'] = preprocess_data(data['train'])\n",
        "data['dev'] = preprocess_data(data['dev'])\n",
        "\n",
        "# Display sample of preprocessed data\n",
        "print(f\"\\nüìù Preprocessed sample:\")\n",
        "print(f\"  {data['train'][0]}\")\n",
        "\n",
        "# Extract vocabulary and mappings\n",
        "chars, tags, char2id = extract_vocab(data['train'])\n"
      ],
      "metadata": {
        "id": "WdXg8jYRt_Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, List, Union\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "@dataclass\n",
        "class TokenizerConfig:\n",
        "    \"\"\"Configuration for CustomTokenizer\"\"\"\n",
        "    bos_token: str = \"<s>\"\n",
        "    eos_token: str = \"</s>\"\n",
        "    unk_token: str = \"<unk>\"\n",
        "    pad_token: str = \"<pad>\"\n",
        "    max_len: int = 512\n",
        "\n",
        "class CustomTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    Custom tokenizer for character-level tokenization with special handling for morphosyntactic features.\n",
        "\n",
        "    Processes input and output word forms character by character while treating morphosyntactic\n",
        "    features as special atomic tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Dict[str, int],\n",
        "        additional_special_tokens: Optional[List[str]] = None,\n",
        "        max_len: int = 512,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocab: Dictionary mapping characters to their IDs\n",
        "            additional_special_tokens: List of special tokens (e.g., morphological tags)\n",
        "            max_len: Maximum sequence length\n",
        "        \"\"\"\n",
        "        config = TokenizerConfig()\n",
        "\n",
        "        # Initialize token mappings\n",
        "        self.__token_ids = vocab.copy()\n",
        "        self.__id_tokens = {v: k for k, v in vocab.items()}\n",
        "\n",
        "        # Process special tokens\n",
        "        special_tokens = {\n",
        "            'pad_token': self._create_added_token(config.pad_token),\n",
        "            'bos_token': self._create_added_token(config.bos_token),\n",
        "            'eos_token': self._create_added_token(config.eos_token),\n",
        "            'unk_token': self._create_added_token(config.unk_token)\n",
        "        }\n",
        "\n",
        "        # Initialize special token decoder\n",
        "        self._added_tokens_decoder = {\n",
        "            i: token for i, token in enumerate(special_tokens.values())\n",
        "        }\n",
        "        self.offset = len(self._added_tokens_decoder)\n",
        "\n",
        "        super().__init__(\n",
        "            **special_tokens,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            max_len=max_len,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_added_token(token: Union[str, AddedToken]) -> AddedToken:\n",
        "        \"\"\"Create an AddedToken with consistent settings.\"\"\"\n",
        "        return AddedToken(token, lstrip=False, rstrip=False) if isinstance(token, str) else token\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Get the size of the vocabulary.\"\"\"\n",
        "        return len(self.__token_ids)\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        \"\"\"Get the full vocabulary including special tokens.\"\"\"\n",
        "        vocab = {\n",
        "            self.convert_ids_to_tokens(i): i\n",
        "            for i in range(self.vocab_size + self.offset)\n",
        "        }\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _add_eos(self, token_ids: List[int]) -> List[int]:\n",
        "        \"\"\"Add end-of-sequence token to the token list.\"\"\"\n",
        "        return token_ids + [self.eos_token_id]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Create token type IDs for single or paired sequences.\"\"\"\n",
        "        total_length = len(token_ids_0) + 1  # +1 for EOS\n",
        "        if token_ids_1:\n",
        "            total_length += len(token_ids_1) + 1\n",
        "        return [0] * total_length\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Build model inputs by adding special tokens.\"\"\"\n",
        "        token_ids_0 = self._add_eos(token_ids_0)\n",
        "        if token_ids_1 is None:\n",
        "            return token_ids_0\n",
        "        return token_ids_0 + self._add_eos(token_ids_1)\n",
        "\n",
        "    def _tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        \"\"\"Tokenize text into characters.\"\"\"\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token to its ID.\"\"\"\n",
        "        return (self.__token_ids.get(token, self.unk_token_id - self.offset)\n",
        "                + self.offset)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        \"\"\"Convert an ID back to its token.\"\"\"\n",
        "        adjusted_index = index - self.offset\n",
        "        return self.__id_tokens.get(adjusted_index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Convert tokens back to string.\"\"\"\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocabulary(self,\n",
        "                       save_directory: str,\n",
        "                       filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        \"\"\"Save the vocabulary to a file.\"\"\"\n",
        "        prefix = filename_prefix or \"\"\n",
        "        vocab_path = Path(save_directory) / f\"{prefix}vocab.json\"\n",
        "\n",
        "        with vocab_path.open('w', encoding='utf-8') as f:\n",
        "            json.dump(self.__token_ids, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return (str(vocab_path),)\n",
        "\n",
        "# Initialize tokenizer with vocabulary and tags\n",
        "tokenizer = CustomTokenizer(char2id, additional_special_tokens=tags, max_len=100)\n",
        "\n",
        "# Display example tokenization\n",
        "sample_idx = 54\n",
        "sample_text = data['train'][sample_idx][0]\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(f\"Tokenization Example:\")\n",
        "print(f\"Input text: {sample_text}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ],
      "metadata": {
        "id": "CpShtsY_uAvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from typing import List, Dict, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for morphological generation task.\n",
        "    Handles tokenization of input lemmas and target forms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: List[Tuple[str, str]],\n",
        "        tokenizer,\n",
        "        max_length: int = 128\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "\n",
        "        Args:\n",
        "            data: List of (input, target) string pairs\n",
        "            tokenizer: Tokenizer for encoding text\n",
        "            max_length: Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return dataset size.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get tokenized item from dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with input_ids and labels tensors\n",
        "        \"\"\"\n",
        "        input_text, target_text = self.data[idx]\n",
        "\n",
        "        # Tokenize input and target\n",
        "        model_inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = self.tokenizer(\n",
        "            target_text,\n",
        "            padding='longest',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": model_inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "def postprocess_data(\n",
        "    token_ids: Union[np.ndarray, torch.Tensor],\n",
        "    tokenizer\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Post-process token IDs into readable text.\n",
        "\n",
        "    Args:\n",
        "        token_ids: Array/Tensor of token IDs\n",
        "        tokenizer: Tokenizer for decoding\n",
        "\n",
        "    Returns:\n",
        "        List of decoded strings with special tokens removed\n",
        "    \"\"\"\n",
        "    # Convert torch tensor to numpy if needed\n",
        "    if isinstance(token_ids, torch.Tensor):\n",
        "        token_ids = token_ids.cpu().numpy()\n",
        "\n",
        "    # Replace padding indices\n",
        "    token_ids = np.where(token_ids != -100, token_ids, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode tokens to strings\n",
        "    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "V06aSc2buCQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation metrics for morphological generation task.\n",
        "Includes exact match scoring and detailed error analysis.\n",
        "\"\"\"\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import Levenshtein\n",
        "from collections import defaultdict\n",
        "\n",
        "@dataclass\n",
        "class PredictionExample:\n",
        "    \"\"\"Store prediction examples for analysis\"\"\"\n",
        "    input: str\n",
        "    prediction: str\n",
        "    target: str\n",
        "    is_correct: bool\n",
        "    edit_distance: int\n",
        "\n",
        "class MetricsComputer:\n",
        "    \"\"\"Compute and analyze model predictions\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, sample_size: int = 15):\n",
        "        \"\"\"\n",
        "        Initialize metrics computer.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: Tokenizer for decoding predictions\n",
        "            sample_size: Number of random examples to sample for analysis\n",
        "        \"\"\"\n",
        "        self.metric = evaluate.load(\"exact_match\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_size = sample_size\n",
        "        self.error_stats = defaultdict(int)\n",
        "\n",
        "    def compute_edit_distance(self, pred: str, target: str) -> int:\n",
        "        \"\"\"Compute Levenshtein distance between prediction and target\"\"\"\n",
        "        return Levenshtein.distance(pred, target)\n",
        "\n",
        "    def analyze_predictions(\n",
        "        self,\n",
        "        decoded_preds: List[str],\n",
        "        decoded_labels: List[str],\n",
        "        inputs: List[str] = None\n",
        "    ) -> List[PredictionExample]:\n",
        "        \"\"\"\n",
        "        Analyze a sample of predictions.\n",
        "\n",
        "        Args:\n",
        "            decoded_preds: List of model predictions\n",
        "            decoded_labels: List of target labels\n",
        "            inputs: Optional list of input texts\n",
        "\n",
        "        Returns:\n",
        "            List of PredictionExample objects\n",
        "        \"\"\"\n",
        "        sample_indices = random.sample(range(len(decoded_preds)),\n",
        "                                     min(self.sample_size, len(decoded_preds)))\n",
        "\n",
        "        examples = []\n",
        "        for idx in sample_indices:\n",
        "            pred = decoded_preds[idx]\n",
        "            target = decoded_labels[idx]\n",
        "            input_text = inputs[idx] if inputs else \"\"\n",
        "\n",
        "            example = PredictionExample(\n",
        "                input=input_text,\n",
        "                prediction=pred,\n",
        "                target=target,\n",
        "                is_correct=pred == target,\n",
        "                edit_distance=self.compute_edit_distance(pred, target)\n",
        "            )\n",
        "            examples.append(example)\n",
        "\n",
        "            # Track error types\n",
        "            if not example.is_correct:\n",
        "                self.error_stats['total_errors'] += 1\n",
        "                if len(pred) != len(target):\n",
        "                    self.error_stats['length_mismatch'] += 1\n",
        "                if pred == target[::-1]:\n",
        "                    self.error_stats['reversed'] += 1\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def compute_metrics(self, eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute evaluation metrics for predictions.\n",
        "\n",
        "        Args:\n",
        "            eval_preds: Tuple of (predictions, labels) arrays\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of metric scores\n",
        "        \"\"\"\n",
        "        preds, labels = eval_preds\n",
        "        decoded_preds = postprocess_data(preds, self.tokenizer)\n",
        "        decoded_labels = postprocess_data(labels, self.tokenizer)\n",
        "\n",
        "        # Compute exact match score\n",
        "        exact_match = self.metric.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels\n",
        "        )\n",
        "\n",
        "        # Analyze sample predictions\n",
        "        examples = self.analyze_predictions(decoded_preds, decoded_labels)\n",
        "\n",
        "        # Compute additional metrics\n",
        "        edit_distances = [ex.edit_distance for ex in examples]\n",
        "\n",
        "        metrics = {\n",
        "            \"exact_match\": exact_match[\"exact_match\"],\n",
        "            \"avg_edit_distance\": np.mean(edit_distances),\n",
        "            \"max_edit_distance\": max(edit_distances),\n",
        "            \"error_rate\": self.error_stats['total_errors'] / len(examples)\n",
        "        }\n",
        "\n",
        "        # Print analysis examples\n",
        "        self._print_analysis(examples)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _print_analysis(self, examples: List[PredictionExample]) -> None:\n",
        "        \"\"\"Print detailed analysis of prediction examples\"\"\"\n",
        "        print(\"\\nüìä Prediction Analysis:\")\n",
        "        print(f\"Analyzing {len(examples)} random examples...\")\n",
        "        print(\"\\nDetailed Examples:\")\n",
        "\n",
        "        for i, ex in enumerate(examples, 1):\n",
        "            status = \"‚úì\" if ex.is_correct else \"‚úó\"\n",
        "            print(f\"\\n{i}. {status} Input: {ex.input}\")\n",
        "            print(f\"   Prediction: {ex.prediction}\")\n",
        "            print(f\"   Target: {ex.target}\")\n",
        "            print(f\"   Edit Distance: {ex.edit_distance}\")\n",
        "\n",
        "        print(\"\\nError Statistics:\")\n",
        "        for error_type, count in self.error_stats.items():\n",
        "            print(f\"  ‚Ä¢ {error_type}: {count}\")\n",
        "\n",
        "# Initialize metrics computer\n",
        "metrics_computer = MetricsComputer(tokenizer)\n",
        "\n",
        "# Use for evaluation\n",
        "compute_metrics = metrics_computer.compute_metrics\n"
      ],
      "metadata": {
        "id": "lj7p5GZ5uDoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {\n",
        "    'train': CustomDataset(data['train'], tokenizer, max_length=128),\n",
        "    'dev': CustomDataset(data['dev'], tokenizer, max_length=128)\n",
        "}"
      ],
      "metadata": {
        "id": "UCagLdF6uGBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the T5-small model and move it to GPU (device 0)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")  # Load pre-trained T5 model\n",
        "model.to(0)  # Move model to first GPU (device ID 0)"
      ],
      "metadata": {
        "id": "PkLXCZebuHQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,  # The tokenizer used to process text\n",
        "    model=model           # The T5 model being used\n",
        ")"
      ],
      "metadata": {
        "id": "mp4cnVjMuKEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training arguments for fine-tuning the T5 model\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    # Directory to save model checkpoints and logs\n",
        "    output_dir=\"/content/drive/My Drive/morphological_inflection_t5\",\n",
        "\n",
        "    # Evaluate model after each epoch\n",
        "    evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # Learning rate for optimization\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch sizes for training and evaluation\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "\n",
        "    # L2 regularization factor (This penalty encourages the model to use smaller weights during training)\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # Keep only the last 3 checkpoints\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Number of training epochs\n",
        "    num_train_epochs=300,\n",
        "\n",
        "    # Enable text generation during evaluation\n",
        "    predict_with_generate=True,\n",
        "\n",
        "    # Enable mixed precision training (faster training, less memory)\n",
        "    fp16=True,\n",
        "\n",
        "    # Disable external reporting\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "dFerhTkpuMZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Sequence-to-Sequence Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                                    # Pre-trained T5 model\n",
        "    args=training_args,                          # Training configuration\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['dev'],\n",
        "    tokenizer=tokenizer,                         # Tokenizer for text processing\n",
        "    data_collator=data_collator,                 # Handles batch preparation\n",
        "    compute_metrics=compute_metrics,             # Evaluation metric function\n",
        ")"
      ],
      "metadata": {
        "id": "UyZmZZtwuNhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "JWKvfZteuOyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(\n",
        "    trainer,\n",
        "    test_path: str,\n",
        "    download_data,\n",
        "    preprocess_data,\n",
        "    tokenizer,\n",
        "    batch_size: int = 32\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data with detailed metrics.\n",
        "\n",
        "    Args:\n",
        "        trainer: Seq2SeqTrainer instance\n",
        "        test_path: URL to test data\n",
        "        batch_size: Batch size for evaluation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and preprocess test data\n",
        "        print(\"Loading test data...\")\n",
        "        test_data = download_data(test_path)\n",
        "        processed_data = preprocess_data(test_data)\n",
        "        test_dataset = CustomDataset(processed_data, tokenizer)\n",
        "\n",
        "        # Run evaluation\n",
        "        print(\"Running evaluation...\")\n",
        "        start_time = time.time()\n",
        "        result = trainer.evaluate(\n",
        "            test_dataset,\n",
        "            max_length=128,\n",
        "            num_beams=5,\n",
        "            metric_key_prefix=\"test\"\n",
        "        )\n",
        "        eval_time = time.time() - start_time\n",
        "\n",
        "        # Add additional metrics\n",
        "        result.update({\n",
        "            \"test_samples\": len(test_dataset),\n",
        "            \"eval_time_seconds\": round(eval_time, 2),\n",
        "            \"eval_samples_per_second\": round(len(test_dataset) / eval_time, 2)\n",
        "        })\n",
        "\n",
        "        # Print detailed results\n",
        "        print(\"\\nüìà Evaluation Results:\")\n",
        "        for metric, value in result.items():\n",
        "            print(f\"  ‚Ä¢ {metric}: {value:.4f}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_model(\n",
        "    trainer=trainer,\n",
        "    test_path=test_data_path,\n",
        "    download_data=download_data,\n",
        "    preprocess_data=preprocess_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Store test accuracy\n",
        "test_accuracy = evaluation_results['test_exact_match']\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "id": "YAv4d4qdvKVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}