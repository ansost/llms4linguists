[
  {
    "objectID": "2024/weeks/week13/slides.html#prompt-tuning",
    "href": "2024/weeks/week13/slides.html#prompt-tuning",
    "title": "Week 03Other Finetuning methdos",
    "section": "Prompt tuning",
    "text": "Prompt tuning\n\nIn a prompt-based LLM, the output is highly dependent on the model weights and the prompt.\nThe idea of prompt tuning is to add a new set of weights between the prompt and the model and only tune those.\nIn the above paper, the T5 model weights were 11,000,000,000 parameters, and the added prompt weights were 20,400 parameters. Yet, similar performance was observed with prompt tuning w.r.t full model weight fine-tuning on a benchmark dataset."
  },
  {
    "objectID": "2024/weeks/week13/slides.html#low-rank-adaptation-of-large-language-models-lora",
    "href": "2024/weeks/week13/slides.html#low-rank-adaptation-of-large-language-models-lora",
    "title": "Week 03Other Finetuning methdos",
    "section": "Low-Rank Adaptation of Large Language Models (LORA)",
    "text": "Low-Rank Adaptation of Large Language Models (LORA)\n\nLORA is a technique to reduce the number of parameters that need to be trained by reducing the dimensions using Single Value Decomposition (SVD).\nDimensionality reduction:\n\nReducing the dimensionality of data by projecting the data onto a lower-dimensional subspace. Mainly used for efficient processing and analysis (visualization purposes).\n\nPaper: https://arxiv.org/pdf/2106.09685\nThe pretrained model parameters are in a super high dimensional space. It is shown that finetuning fewer parameters in a compressed space can drastically affect the final output. This trick is used to creating a new set of LORA parameters that are much smaller than the others."
  },
  {
    "objectID": "2024/weeks/week13/slides.html#section",
    "href": "2024/weeks/week13/slides.html#section",
    "title": "Week 03Other Finetuning methdos",
    "section": "",
    "text": "Let‚Äôs check the figure 1 from the above paper‚Ä¶\n\nwhere A and B are the LORA parameters. The dimension x is the original input dimension and r is the reduced dimension. During fine tuning only A and B are adjusted enabling the model to learn task specific features.\n\nThese LORA weights can be swapped in and out for different tasks and/or different LORA weights can be stored as different model implementations.\n\nFurther reading:\n\nhttps://huggingface.co/docs/peft/main/en/conceptual_guides/lora\nhttps://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\nhttps://web.stanford.edu/class/cs168/l/l9.pdf"
  },
  {
    "objectID": "2024/weeks/week13/slides.html#qlora",
    "href": "2024/weeks/week13/slides.html#qlora",
    "title": "Week 03Other Finetuning methdos",
    "section": "QLoRa",
    "text": "QLoRa\n\nTypically, the parameters of pre-trained models are stored in a 32-bit format, and QLoRa compresses them to 4-bit format. This reduces the memory footprint and enables finetuning using much less resources. Each parameter in the model is represented by 32 bits (float32). One float32 has 32 bits which is 4 bytes. So it requires 4 gigabytes for one billion parameter model trained on FP32.\nPost-Training Quantization (PTQ): quantization on LLM after it has been trained.\nQuantization-Aware Training (QAT): quantization on LLM during training.\nPaper: https://arxiv.org/pdf/2305.14314"
  },
  {
    "objectID": "2024/weeks/week13/slides.html#distillation",
    "href": "2024/weeks/week13/slides.html#distillation",
    "title": "Week 03Other Finetuning methdos",
    "section": "Distillation",
    "text": "Distillation\n\nDistillation is the process of using the generated output or generation to finetune the model.\nKnowledge Distillation: Take a big model and make it teach a smaller model.\n\nPaper: https://arxiv.org/pdf/2306.08543\n\nContext distillation: Prompting the model with a prompt prefix (‚Äúanalyze like a linguist‚Äù and then training on the model response.\n\nPaper: https://arxiv.org/pdf/2209.15189"
  },
  {
    "objectID": "2024/weeks/week13/slides.html#prompt-engineering",
    "href": "2024/weeks/week13/slides.html#prompt-engineering",
    "title": "Week 03Other Finetuning methdos",
    "section": "Prompt engineering",
    "text": "Prompt engineering\n\nHere, the model is prompted to take up roles, such as act as an ‚Äúexpert‚Äù or ‚Äúactor‚Äù or ‚Äúbe a linguist‚Äù. This cannot be categorized as finetuning because no weights are updated.\nAs this method doesn‚Äôt require any additional compute at train time, it is easy to iterate upon. But this method does not affect the LLM behaviour as strongly as fine-tuning.\nKnow more: https://huggingface.co/docs/peft/conceptual_guides/prompting"
  },
  {
    "objectID": "2024/weeks/week13/slides.html#thank-you",
    "href": "2024/weeks/week13/slides.html#thank-you",
    "title": "Week 03Other Finetuning methdos",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "2024/weeks/week11/page.html",
    "href": "2024/weeks/week11/page.html",
    "title": "üóìÔ∏è Week 11 - Morphological inflection task with T5 models using CHILDES datasets",
    "section": "",
    "text": "Click here to download the notebook.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 11 - morphological inflection with T5 using CHILDES"
    ]
  },
  {
    "objectID": "2024/weeks/week11/page.html#lecture",
    "href": "2024/weeks/week11/page.html#lecture",
    "title": "üóìÔ∏è Week 11 - Morphological inflection task with T5 models using CHILDES datasets",
    "section": "",
    "text": "Click here to download the notebook.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 11 - morphological inflection with T5 using CHILDES"
    ]
  },
  {
    "objectID": "2024/weeks/week11/page.html#homework",
    "href": "2024/weeks/week11/page.html#homework",
    "title": "üóìÔ∏è Week 11 - Morphological inflection task with T5 models using CHILDES datasets",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 11 - morphological inflection with T5 using CHILDES"
    ]
  },
  {
    "objectID": "2024/weeks/week09/page.html",
    "href": "2024/weeks/week09/page.html",
    "title": "üóìÔ∏è Week 09 - Finetuning t5 model for machine translation",
    "section": "",
    "text": "Click here to download thenotebook.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 09 - finetuning machine translation"
    ]
  },
  {
    "objectID": "2024/weeks/week09/page.html#lecture",
    "href": "2024/weeks/week09/page.html#lecture",
    "title": "üóìÔ∏è Week 09 - Finetuning t5 model for machine translation",
    "section": "",
    "text": "Click here to download thenotebook.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 09 - finetuning machine translation"
    ]
  },
  {
    "objectID": "2024/weeks/week09/page.html#homework",
    "href": "2024/weeks/week09/page.html#homework",
    "title": "üóìÔ∏è Week 09 - Finetuning t5 model for machine translation",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 09 - finetuning machine translation"
    ]
  },
  {
    "objectID": "2024/weeks/week07/page.html",
    "href": "2024/weeks/week07/page.html",
    "title": "üóìÔ∏è Week 07 - Implementing a GPT-2 model",
    "section": "",
    "text": "Click here to download the implementation of GPT-2 model.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 07 - GPT-2 implementation"
    ]
  },
  {
    "objectID": "2024/weeks/week07/page.html#lecture",
    "href": "2024/weeks/week07/page.html#lecture",
    "title": "üóìÔ∏è Week 07 - Implementing a GPT-2 model",
    "section": "",
    "text": "Click here to download the implementation of GPT-2 model.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 07 - GPT-2 implementation"
    ]
  },
  {
    "objectID": "2024/weeks/week07/page.html#homework",
    "href": "2024/weeks/week07/page.html#homework",
    "title": "üóìÔ∏è Week 07 - Implementing a GPT-2 model",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 07 - GPT-2 implementation"
    ]
  },
  {
    "objectID": "2024/weeks/week05/slides.html#layer-normalization",
    "href": "2024/weeks/week05/slides.html#layer-normalization",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "Layer normalization",
    "text": "Layer normalization\n\nLayer normalization is applied to the output of the feed-forward neural network.\nLayer normalization is a technique that normalizes the output of each layer in the transformer.\nNormalization is a technique used to scale the input features to a range that is more suitable for the model to learn.\nIt helps in stabilizing the training process and speeds up convergence.\n\n\\[\n\\begin{align*}\nLayerNorm(x) = \\frac{x - \\mu}{\\sigma}\n\\end{align*}\n\\]\nwhere,\n\n\\(\\mu\\) is the mean of the input vector \\(x\\).\n\\(\\sigma\\) is the standard deviation of the input vector \\(x\\)."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section",
    "href": "2024/weeks/week05/slides.html#section",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Residual connection\n\nDuring the process of training deep networks, it has been observed that the learning can stagnate or become more difficult for longer input sequences.\nThis makes it difficult for the model to capture long range dependencies.\nTo address this issue, the transformer uses residual connections.\nThey are used around the multi-head attention and feed-forward neural network sub-layers.\n\n\\[\n\\begin{align*}\n\\text{Output} = \\text{Input} + \\text{Sublayer}(\\text{Input})\n\\end{align*}\n\\]\nwhere,\n\n\\(\\text{Input}\\) is the input to the sub-layer.\n\\(\\text{Sublayer}\\) is the sub-layer that consists of multi-head attention or feed-forward neural network."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#position-wise-feed-forward-neural-network",
    "href": "2024/weeks/week05/slides.html#position-wise-feed-forward-neural-network",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "Position-wise feed-forward neural network",
    "text": "Position-wise feed-forward neural network\n\nA fully connected feed-forward neural network sub-layer is a point-wise function, which means that it processes each word vector independently.\nThe feed-forward neural network consists of two linear transformations with a ReLU activation function in between.\nThis sub-layer introduces additional non-linearity and allows the model to learn complex relationships between the input and output.\n\n\\[\n\\begin{align*}\nFF(x) = f(x \\cdot K^T) \\cdot V\n\\end{align*}\n\\]\nwhere,\n\n\\(K\\) and \\(V\\) are the weight matrices for the feed-forward neural network.\n\\(f\\) is the ReLU activation function.\n\\(x\\) is the input to the feed-forward neural network."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-1",
    "href": "2024/weeks/week05/slides.html#section-1",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Masked multi-head self-attention layer\n\nThe masked self-attention mechanism allows the model to only attend to words that have been generated so far and prevents it from attending future words that have not been predicted yet.\nThis is achieved by applying a mask to the attention weights.\nA mask matrix \\(M\\) is created of the same size as the attention scores\nThis mask matrix is added element-wise to the attention scores.\n\n\\[\n\\begin{align*}\nMaskedScores_{ij} = A_{ij} + M_{ij}\n\\end{align*}\n\\]\nwhere,\n\n\\(MaskedScores_{ij}\\) is the masked attention score for the i-th query and j-th key.\n\\(A_{ij}\\) is the attention score for the i-th query and j-th key.\n\\(M_{ij}\\) is the mask matrix."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-2",
    "href": "2024/weeks/week05/slides.html#section-2",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "The \\(MaskedScores_{ij}\\) are passed through softmax function to get the attention weights.\nThis softmax function will assign nearly zero weights to the positions with \\(-inf\\) values and thereby masking them out.\nAs a final step, weighted sum is calculated to get the output of the masked self-attention mechanism.\nThis output is then passed to the encoder-decoder attention layer.\nDuring inference, the model generates each word one at a time and the masking is applied to prevent attending to future positions.\nIn other words, each predicted word is conditioned only on the previously generated words and is referred to as autoregressive generation."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#encoder-decoder-attention-layer",
    "href": "2024/weeks/week05/slides.html#encoder-decoder-attention-layer",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "Encoder-decoder attention layer",
    "text": "Encoder-decoder attention layer\n\nThe Encoder-decoder attention layer allows the decoder to combine the encoded sequence of the encoder with the output generated from the multi-head self-attention layer.\nAt each time step \\(t\\) in the decoder, the following computations are performed: Compute the query vector \\(Q_t\\) using the current decoder input embedding \\(X_t\\):\n\n\\[\nQ_t = X_t \\cdot W^Q\n\\]\nwhere,\n\n\\(W^Q\\) is the weight matrix for the query vector.\n\\(X_t\\) is the input to the decoder at time step \\(t\\).\n\\(Q_t\\) is the query vector for the t-th time step."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-3",
    "href": "2024/weeks/week05/slides.html#section-3",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Next, the keys \\(K\\) and values \\(V\\) from the encoder output sequence \\(H\\) are computed using:\n\n\\[\n\\begin{align*}\n\\text{K} & = H \\cdot W^K \\\\\n\\text{V} & = H \\cdot W^V \\\\\n\\end{align*}\n\\]\nwhere,\n\n\\(W^K\\) and \\(W^V\\) are the weight matrices for the keys and values.\n\\(H\\) is the output of the encoder.\n\\(X_t\\) is the input to the decoder at time step \\(t\\)."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-4",
    "href": "2024/weeks/week05/slides.html#section-4",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "The attention score is calculated by taking the dot product of the query vector \\(Q_t\\) and the key vector \\(K\\) in the encoder output sequence \\(H\\).\nThe attention score is then scaled and passed through a softmax function to get the attention weights.\nThe weighted sum is calculated to get the output of the encoder-decoder attention layer.\nLastly, a context vector \\(Context_t\\) is computed to capture the relevant information from the input sequence that the decoder should focus on when generating the output token at time step \\(t\\)."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-5",
    "href": "2024/weeks/week05/slides.html#section-5",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "This is computed by taking the weighted sum of the value vectors \\(V\\) using the attention weights:\n\\[\n\\begin{align*}\nContext_t = \\sum_{j=1}^{n} \\alpha_{tj} \\cdot v_j\n\\end{align*}\n\\]\nwhere,\n\n\\(\\alpha_{tj}\\) is the attention weight for the t-th query and j-th key.\n\\(v_j\\) is the value vector for the j-th key.\nThis context vector is concatenated with the decoder input embeddings \\(X_t\\) and passed through a linear layer to get the output of the encoder-decoder attention layer.\nThis output is then passed to the point-wise feed-forward neural network layer."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-6",
    "href": "2024/weeks/week05/slides.html#section-6",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Point-wise feed-forward neural network layer\n\nThe feed-forward neural network within the decoder operates in the same manner to that within the encoder.\nHowever, there is a key difference in the input to the feed-forward network in the decoder.\nThe input to the point-wise feed-forward network comes from the encoder-decoder layer.\n\nLayer normalization\n\nEach sub-layer (masked multi-head self-attention, encoder-decoder attention, and position-wise feed-forward network) is followed by a layer normalization operation and connected with a residual connection\nThe layer normalization stabilizes training and enables the model to learn more effectively."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-7",
    "href": "2024/weeks/week05/slides.html#section-7",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Residual connection\n\nThe residual connection is applied around each sub-layer in the decoder.\nThey are applied around the masked multi-head self-attention layer, encoder-decoder attention layer and point-wise feed-forward neural network layer.\n\nLinear or Output layer\n\nThe linear or the output layer determines the likelihood of each word being the next word in the output sequence.\nThe input to this layer is the output from the layer normalization.\nThe purpose of the linear layer is to map the output from the layer normalization to a vector of raw scores (logits) corresponding to each word being the next word in the output sequence.\nThis done by applying a linear transformation, which involves multiplying the output from the layer normalization layer by a weight matrix and adding a bias vector."
  },
  {
    "objectID": "2024/weeks/week05/slides.html#section-8",
    "href": "2024/weeks/week05/slides.html#section-8",
    "title": "Week 05 Intro to Transformers (part 2)",
    "section": "",
    "text": "Softmax\n\nThe output of the linear layer is passed through a softmax function to get the probability distribution over the output vocabulary.\nDuring the training phase, the predicted probabilities are used to compute the cross-entropy loss function, which measures the dissimilarity between the predicted distribution and the true distribution.\nDuring the inference phase, the word with the highest probability at each position is chosen as the predicted output."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#converting-words-to-vectors",
    "href": "2024/weeks/week04/slides.html#converting-words-to-vectors",
    "title": "Week 04 Intro to Transformers",
    "section": "Converting words to vectors",
    "text": "Converting words to vectors\n\nMachines cannot understand words directly, they can only understand numbers.\nWords to vectors is a process of converting words into numerical vectors.\nCan you think of ways to convert words to vectors?\nHow would you represent the sentence ‚ÄúI am a student‚Äù as a vector?"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section",
    "href": "2024/weeks/week04/slides.html#section",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "One-hot encoding\n\nOne-hot encoding is the simplest way to convert words to vectors.\nEach word is represented as a vector of zeros with a 1 at the index corresponding to the word.\nFor example, the sentence ‚ÄúI am a student‚Äù can be represented as a matrix of one-hot encoded vectors.\n‚ÄúI‚Äù is represented as [1, 0, 0, 0], ‚Äúam‚Äù is represented as [0, 1, 0, 0], and so on."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-1",
    "href": "2024/weeks/week04/slides.html#section-1",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Limitations of one-hot encoding\n\nOne-hot encoding has several limitations:\n\nIt does not capture the relationships between words.\nIt does not consider the context in which the words appear.\nIt does not account for the similarity between words.\n\n\nWord embeddings\n\nWord embeddings are dense vector representations of words that capture the relationships between words.\nWord embeddings are learned from large text corpora using neural networks.\nFor example, the word ‚Äúking‚Äù might be represented as [0.2, 0.3, 0.5], and the word ‚Äúqueen‚Äù might be represented as [0.1, 0.4, 0.6]. These vectors capture the relationship between the two words.\nThese vectors are learned in such a way that similar words have similar embeddings."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#transformer-architecture",
    "href": "2024/weeks/week04/slides.html#transformer-architecture",
    "title": "Week 04 Intro to Transformers",
    "section": "Transformer architecture",
    "text": "Transformer architecture\n\nImage from ‚ÄúAttention is all you need‚Äù"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-2",
    "href": "2024/weeks/week04/slides.html#section-2",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "In a transformer-based encoder-decoder architecture, the transformer consists of a encoder block and a decoder block\nThe encoder block consists of a stack of N=6 identical layers\nThe decoder block consists of a stack of N=6 identical layers\nInput sequence is passed through the encoder block to generate a sequence of hidden states which are then passed through the decoder block to generate the output sequence\nFor example, input sequence is a sentence in English and output sequence is a sentence in German. ‚ÄúI am a student‚Äù -&gt; ‚ÄúIch bin ein Student‚Äù"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#embedding",
    "href": "2024/weeks/week04/slides.html#embedding",
    "title": "Week 04 Intro to Transformers",
    "section": "Embedding",
    "text": "Embedding\n\nEach word in the input and output sequences is represented as a 512-dimensional vector called an embedding.\nThe embedding layer maps each word to its corresponding embedding vector.\nBefore training begins, each word is assigned a random embedding vector.\nThese are small random values obtained from a normal distribution.\nThe model tries to capture the patterns and dependencies between words by continuously updating this embedding during training."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#positional-encoding",
    "href": "2024/weeks/week04/slides.html#positional-encoding",
    "title": "Week 04 Intro to Transformers",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nPositional encoding is added to the input embeddings to give the model information about the position of each word in the sequence.\nUnlike humans who naturally read from left to right, the transformer needs a special way to understand that ‚Äúword 1 comes before word 2.‚Äù\nThe positional encoding in the original transformer is implemented using sine function of different frequencies.\nUsing sine waves makes it easier for the transformer to understand both nearby and far-apart relationships between words. (It‚Äôs similar to how music uses different frequencies to create unique sounds.)\nThe positional encoding vectors have the same dimensions as the embedding vectors and are added element-wise to create the input representation for each word.\nThis allows the model to differentiate between words based on their position in the sequence."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-3",
    "href": "2024/weeks/week04/slides.html#section-3",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Before diving into multi-head attention, it is important to understand self-attention mechanism, which forms the foundation of multi-head attention.\nIn the transformer currently implemented, the self-attention mechanism is applied for each word in the sequence."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#self-attention",
    "href": "2024/weeks/week04/slides.html#self-attention",
    "title": "Week 04 Intro to Transformers",
    "section": "Self-Attention",
    "text": "Self-Attention\n\nSelf-attention is a mechanism that helps the model weigh the importance of different words in the input sequence when generating each output word.\nIt is a mechanism that allows the model to focus on different parts of the input sequence when generating each output word.\nThe self-attention mechanism is applied to each word in the input sequence."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-4",
    "href": "2024/weeks/week04/slides.html#section-4",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "The self-attention mechanism has the following steps:\n\nLinear Transformation:\n\nA linear transformation is applied to the input representation (obtained from the Embedding and Positional encoding) to get query vector (Q), key vector (K) and value vector (V) for each word.\nQuery vectors are responsible for expressing what the model is currently looking for in the input sequence.\nKey vectors have representations which provide information of inter-word dependencies and connections between words.\nValue vectors contain additional information of each word in the input sequence."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-5",
    "href": "2024/weeks/week04/slides.html#section-5",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Given an input sequence, \\[\n\\begin{align*}\n\\text{X} & = [x_1, x_2, x_3, \\ldots, x_n] \\\\\n\\end{align*}\n\\]\nThe linear transformations are expressed as:\n\\[\n\\begin{align*}\n\\text{Q} & = \\text{X} \\cdot \\text{W}^Q \\\\\n\\text{K} & = \\text{X} \\cdot \\text{W}^K \\\\\n\\text{V} & = \\text{X} \\cdot \\text{W}^V \\\\\n\\end{align*}\n\\]\nwhere, \\(\\text{W}^Q\\), \\(\\text{W}^K\\) and \\(\\text{W}^V\\) are the weight matrices for the query, key and value vectors respectively. Q, K and V are the query, key and value matrices respectively.\n\nThe same linear transformation is applied to all words in the input sequence.\nThrough the linear tranformation, the input word embeddings are mapped to three different contexts: Query, Key and Value."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-6",
    "href": "2024/weeks/week04/slides.html#section-6",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Scaled Dot-Product Attention: After the linear transformation, the model computes attention scores by calculating the dot products of each element in the query vector and the key vector, scaling them and applying a softmax function to get the attention weights.\n\n\nDot-product\n\nGiven the set of query vectors,\n\\[\n\\begin{align*}\n\\text{Q} & = [q_1, q_2, q_3, \\ldots, q_n] \\\\\n\\end{align*}\n\\]\nGiven the set of key vectors,\n\\[\n\\begin{align*}\n\\text{K} & = [k_1, k_2, k_3, \\ldots, k_n] \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-7",
    "href": "2024/weeks/week04/slides.html#section-7",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "The attention score matrix \\(\\text{A}\\) is a matrix where each entry \\(A_{ij}\\) is the dot product of the i-th query and and j-th key.\n\\[\n\\begin{align*}\nA_{ij} & = q_i \\cdot k_j \\\\\n\\end{align*}\n\\]\n\nScaling\n\n\nThe dot-products from above, can potentially become very large.\nLarge values affect the training by causing issues in softmax (as these large values might exceed the representable range of numerical precision of the computer, which leads to incorrect outputs).\nSo, the dot-products are scaled by the square root of the dimension of the key vectors (\\(d_{k}\\)).\n\n\\[\n\\begin{align*}\nA_{ij} & = \\frac{q_i \\cdot k_j}{\\sqrt{d_{k}}} \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-8",
    "href": "2024/weeks/week04/slides.html#section-8",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Softmax\n\n\nThe scaled dot-products are passed through a softmax function to get the attention weights.\n\n\\[\n\\begin{align*}\n\\alpha_{ij} & = \\frac{\\exp(A_{ij})}{\\sum_{j=1}^{n} \\exp(A_{ij})}\n\\end{align*}\n\\]\nexplained as: where,\n\n\\(\\alpha_{ij}\\) is the attention weight for the i-th query and j-th key.\n\\(\\exp\\) is the exponential function (e=2.71828).\nThe softmax function is applied to each row of the attention score matrix \\(\\text{A}\\)."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#section-9",
    "href": "2024/weeks/week04/slides.html#section-9",
    "title": "Week 04 Intro to Transformers",
    "section": "",
    "text": "Weighted Sum\n\n\nThe weighted sum is the sum of the element-wise product of the attention weights and the corresponding value vector.\nThe weighted sum is the output of the self-attention mechanism.\n\n\\[\n\\begin{align*}\n\\text{O} = \\sum_{j=1}^{n} \\alpha_{ij} \\cdot v_j\n\\end{align*}\n\\]\nwhere,\n\n\\(\\text{O}\\) is the output of the self-attention mechanism.\n\\(\\alpha_{ij}\\) is the attention weight for the i-th query and j-th key.\n\\(v_j\\) is the value vector for the j-th key."
  },
  {
    "objectID": "2024/weeks/week04/slides.html#multi-headed-attention",
    "href": "2024/weeks/week04/slides.html#multi-headed-attention",
    "title": "Week 04 Intro to Transformers",
    "section": "Multi-headed attention",
    "text": "Multi-headed attention\n\nThe self-attention mechanism is applied multiple times in parallel to the input sequence.\n‚ÄúHead‚Äù refers to an individual component in the multi-head self-attention mechanism that independently learns different self-attention patterns.\nThis allows the model to focus on multiple parts of the input sequence in parallel and thereby allowing the model to capture the entire context.\nThis also makes the model more computationally efficient as it enables parallel processing across different heads.\n\nThe outputs from all the attention heads are mapped to a linear layer:\n\\[\n\\begin{align*}\n\\text{O} & = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) \\cdot \\text{W}^O \\\\\n\\end{align*}\n\\]\nwhere, \\(\\text{W}^O\\) is the weight matrix for the output of the multi-head attention mechanism.\n\nThe outputs are then passed to a feed-forward neural network."
  },
  {
    "objectID": "2024/assignments/assignment02.html",
    "href": "2024/assignments/assignment02.html",
    "title": "Assignment 2: Morphological inflection task using the T5 model",
    "section": "",
    "text": "Here‚Äôs the google colab notebook link for the assignment: https://colab.research.google.com/drive/1XdFUfr8CNEyw0lGzMmlBNnlN2J57n4Z8?usp=sharing\nThe notebook contains the instructions for the assignment.",
    "crumbs": [
      "‚úçÔ∏è Assignments   ",
      "üìù Assignment 02"
    ]
  },
  {
    "objectID": "2024/homework/homework03.html",
    "href": "2024/homework/homework03.html",
    "title": "Homework 3: Intro to transformers",
    "section": "",
    "text": "Reading\n\nRead the first 8 pages of Attention is All You Need paper.\nRead about Self-attention mechanism here: https://jalammar.github.io/illustrated-transformer/\nWatch these videos:\n\nHow large language models work, a visual intro to transformers\nAttention in transformers, visually explained\nIllustrated Guide to Transformers Neural Network: A step by step explanation",
    "crumbs": [
      "üß© Homework",
      "Hw 03 - Intro to transformers"
    ]
  },
  {
    "objectID": "2024/homework/homework01.html",
    "href": "2024/homework/homework01.html",
    "title": "Homework 1: Using Google Colab",
    "section": "",
    "text": "The instructions below assume that you have a Google account or are willing to make one. If you do not want to make a Google account,  contact Anna for instructions to run notebooks without Colab.",
    "crumbs": [
      "üß© Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/homework/homework01.html#google-colab",
    "href": "2024/homework/homework01.html#google-colab",
    "title": "Homework 1: Using Google Colab",
    "section": "Google Colab",
    "text": "Google Colab\n\nGo to the notebooks folder. Click on the three dots for the hw1.ipynb file, select ‚ÄúOpen with‚Äù and then ‚ÄúOpen in Colab‚Äù.\nRun both cells in the notebook by clicking on the play icon on the left of the cell or clicking into the cell and pressing CTRL + ENTER on Windows or CMD + ENTER on Mac.",
    "crumbs": [
      "üß© Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/weeks/week03/page.html",
    "href": "2024/weeks/week03/page.html",
    "title": "üóìÔ∏è Week 03 - Basics of Neural Networks (part 2)",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 03 - NN basics II"
    ]
  },
  {
    "objectID": "2024/weeks/week03/page.html#lecture-slides",
    "href": "2024/weeks/week03/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 03 - Basics of Neural Networks (part 2)",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 03 - NN basics II"
    ]
  },
  {
    "objectID": "2024/weeks/week03/page.html#homework",
    "href": "2024/weeks/week03/page.html#homework",
    "title": "üóìÔ∏è Week 03 - Basics of Neural Networks (part 2)",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 03 - NN basics II"
    ]
  },
  {
    "objectID": "2024/weeks/week03/page.html#recommended-reading",
    "href": "2024/weeks/week03/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 03 - Basics of Neural Networks (part 2)",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 03 - NN basics II"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html",
    "href": "2024/weeks/week02/page.html",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "",
    "text": "In this second week, we will cover a few basic math concepts used in neural networks, try to get an intuition of what machine learning models are and finally explain the architecture a perceptron.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#lecture-slides",
    "href": "2024/weeks/week02/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#homework",
    "href": "2024/weeks/week02/page.html#homework",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\n\nPlease go through the slides and the recommended reading material.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#recommended-reading",
    "href": "2024/weeks/week02/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nWikipedia articles on Matrix multiplication, matrix addition, element-wise multiplication, and dot product.\nWhat is a neural network? [Video]",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html",
    "href": "2024/weeks/week01/page.html",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#lecture-slides",
    "href": "2024/weeks/week01/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#homework",
    "href": "2024/weeks/week01/page.html#homework",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\nWeek 01 - Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#recommended-reading",
    "href": "2024/weeks/week01/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nSpeech and Language Processing, Chapter 3: N-gram language models PDF. Authors: Dan Jurafsky and James H. Martin.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/homework.html",
    "href": "2024/homework.html",
    "title": "Homework Overview",
    "section": "",
    "text": "Here is an overview of all Homework up to date:\nHw 1: Google Colab: here\nHw 2: Neural Networks: here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLMs in Linguistic Research",
    "section": "",
    "text": "InstructorTeaching Staff\n\n\n Akhilesh Kakolu Ramarao  üìß kakolura at hhu dot de\n\n\n Anna Stein  üìß anna.stein at hhu dot de\n\n\n\n\nSyllabus\nView it online here.\n\n\nüìç Location\nWednesday, 2:30pm-4pm\nBuilding 23.21 Floor 2, Room 22\n\n\nüéØ Learning Objectives\nFocus: Understanding LLMs and their usage in various linguistics tasks\nHow: Hands-on learning and paper discussions\n\nGaining an intuition of how LLMs work\nGetting to know the field of LLM research and development\nFamiliarizing yourself with different LLM architectures\nLearning how to use them for different linguistics tasks (e.g., text analysis, language generation, and language understanding)\nPracting using tools like Huggingface for LLM inference and fine-tuning",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "2024/assignments.html",
    "href": "2024/assignments.html",
    "title": "Assignment Overview",
    "section": "",
    "text": "Here is an overview of all assignments up to date:\nAssignment 1: TBD\nAssignment 2: TBD\nAssignment 3: TBD\nAssignment 4: TBD\nAssignment 5: TBD",
    "crumbs": [
      "‚úçÔ∏è Assignments   "
    ]
  },
  {
    "objectID": "2024/syllabus.html",
    "href": "2024/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Last changed: 23.10.24\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Perceptrons and Neural Networks (part 2)\n\nAssignment 1\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD",
    "crumbs": [
      "üìì Syllabus"
    ]
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "href": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Akhilesh",
    "text": "WhoamI: Akhilesh\n\n\n\n\n\nAkhilesh Kakolu Ramarao (he/him) PhD researcher at the English Language and Linguistics department\n\n\n\n\nResearching Computational Morphology supervised by Prof.¬†Dr.¬†Kevin Tang and Dr.¬†Dinah Baer-Henney\nStarted at HHU in 2021\nIndustry Background: NLP Researcher, Software Engineer, Building multilingual chatbots, voice assistants\nInvolved in several language revitalization efforts for indigenous communities in Arunachal Pradesh, India, with a focus on Idu Mishmi and K‚Äôman Mishmi languages\nMore: https://akkikek.xyz/about.html\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-anna",
    "href": "2024/weeks/week01/slides.html#whoami-anna",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Anna",
    "text": "WhoamI: Anna\n\n\n\n\n\nAnna Sophia Stein (she/her)  MSc Linguistics student Focus on Computational Linguistics üìß anna.stein@hhu.de \n\n\n\n\nMain interests: anything Natural Language Processing, open source, ‚Ä¶\nPart of research at the Anglistics, Linguistics and Computer Science department\nCurrently writing my MA thesis on making LLMs better at pragmatics\n\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#contact",
    "href": "2024/weeks/week01/slides.html#contact",
    "title": "Week 01 Introduction",
    "section": "Contact",
    "text": "Contact\n\nüìß: kakolura@hhu.de\nüè¢üï∞Ô∏è: By appointment\nüåêüë•: https://slam.phil.hhu.de\nüê¶: SLaMLab_HHU\nüêôüê±: https://github.com/hhuslamlab/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview",
    "href": "2024/weeks/week01/slides.html#syllabus-overview",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Recurrent Neural Networks\n\nAssignment 1"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#statistical-language-models",
    "href": "2024/weeks/week01/slides.html#statistical-language-models",
    "title": "Week 01 Introduction",
    "section": "Statistical Language Models",
    "text": "Statistical Language Models\n\nThese models use statistical patterns in the data to make predictions about the likelihood of specific sequences of words\nN-gram models are the most common type of statistical language model that predicts the probability of a word given the previous n-1 words\nThey are simple and computationally efficient but have limitations in capturing long-range dependencies\nThey are widely used in speech recognition, machine translation, and other NLP tasks\nThey are used as a baseline for more complex language models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "href": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "title": "Week 01 Introduction",
    "section": "Probabilistic Language Models",
    "text": "Probabilistic Language Models\n\nThese models assign a probability to sequences of words based on the training data\nThey are based on the principles of probability theory and use probabilistic methods to model the language\nThey can capture complex patterns in the data and are more flexible than statistical models\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and speech recognition"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#neural-language-models",
    "href": "2024/weeks/week01/slides.html#neural-language-models",
    "title": "Week 01 Introduction",
    "section": "Neural Language Models",
    "text": "Neural Language Models\n\nThese models use neural networks to predict the likelihood of a sequence of words\nThey are trained on a large corpus of text data and can learn complex patterns and dependencies\nThey are more powerful than traditional statistical and probabilistic models\nThey are used in a wide range of NLP tasks, such as, machine translation, text generation, and sentiment analysis"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#large-language-models",
    "href": "2024/weeks/week01/slides.html#large-language-models",
    "title": "Week 01 Introduction",
    "section": "Large Language Models",
    "text": "Large Language Models\n\nThey are advanced language models that handle billions of training data parameters and generate text output\nThey are trained on large-scale datasets and can generate human-like text\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and question answering\nThey are the focus of the course"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#thank-you",
    "href": "2024/weeks/week01/slides.html#thank-you",
    "title": "Week 01 Introduction",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section",
    "href": "2024/weeks/week02/slides.html#section",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Scalars: single number\n\\[\nx = 1\n\\]\nVectors: sequence of numbers\n\\[\nv = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nMatrix: 2D list of numbers\n\\[\nM = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-1",
    "href": "2024/weeks/week02/slides.html#section-1",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Matrix multiplication\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 22 & 28 \\\\ 49 & 64 \\end{bmatrix}\n\\]\nExplanation:\n\nThe first matrix has 2 rows and 3 columns, and the second matrix has 3 rows and 2 columns.\nThe number of columns in the first matrix should be equal to the number of rows in the second matrix.\nThe resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\nYou multiply the rows of the first matrix with the columns of the second matrix."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-2",
    "href": "2024/weeks/week02/slides.html#section-2",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Element-wise multiplication\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\odot \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 1 & 4 & 9 \\\\ 16 & 25 & 36 \\end{bmatrix}\n\\]\n\nThe matrices should have the same dimensions.\nThe resulting matrix will have the same dimensions as the input matrices.\nYou multiply the corresponding elements of the matrices."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-3",
    "href": "2024/weeks/week02/slides.html#section-3",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Matrix addition\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 2 & 4 & 6 \\\\ 8 & 10 & 12 \\end{bmatrix}\n\\]\n\nYou add the corresponding elements of the matrices.\nThe matrices should have the same dimensions.\nThe resulting matrix will have the same dimensions as the input matrices."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-4",
    "href": "2024/weeks/week02/slides.html#section-4",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Dot product\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\n\\[\n= 1 \\times 1 + 2 \\times 2 + 3 \\times 3 = 14\n\\]\n\nThe number of columns in the first matrix should be equal to the number of rows in the second matrix.\nThe resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\nYou multiply the corresponding elements of the matrices and sum them up."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-5",
    "href": "2024/weeks/week02/slides.html#section-5",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Let us say that you are given a set of inputs and outputs. You need to find how the inputs are related to the outputs.\nInputs: \\(0,1,2,3,4\\)\nOutputs: \\(0,2,4,6,8\\)\n\nYou can see that the output is twice the input.\nThis is a simple example of a relationship between inputs and outputs.\nYou can use this relationship to predict the output for new inputs.\n\nConsider a more complex relationship between inputs and outputs.\nInputs: \\(0,1,2,3,4\\)\nOutputs: \\(0,1,1,2,3\\)\n\nCan you find the relationship between the inputs and outputs?\nThis is where machine learning comes into play.\nMachine learning algorithms can learn the relationship between inputs and outputs from the data."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#neural-networks",
    "href": "2024/weeks/week02/slides.html#neural-networks",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nNeural networks are a class of machine learning models inspired by the human brain.\n\nLearning Alorithm\n\nNeural networks learn by looking at many examples.\nThey adjust their internal settings (= parameters) to improve their accuracy.\nThis is process is called training.\n\nAdvantages of neural networks\n\nCan learn complex patterns.\nCan generalize to new data.\nCan be used for a wide range of tasks (speech recognition, and natural language processing)."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#architecture",
    "href": "2024/weeks/week02/slides.html#architecture",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Architecture",
    "text": "Architecture\n   The input can be a vector, and the output some classification, like a corresponding animal."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-6",
    "href": "2024/weeks/week02/slides.html#section-6",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Every Neural Network has Layers. They are responsible for a specific action, like addition, and pass information to eachother."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-7",
    "href": "2024/weeks/week02/slides.html#section-7",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Layers consist of neurons which each modify the input in some way."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-8",
    "href": "2024/weeks/week02/slides.html#section-8",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "The simplest Neural network only has one layer with one neuron. This is called a perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#architecture-perceptron",
    "href": "2024/weeks/week02/slides.html#architecture-perceptron",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Architecture: Perceptron",
    "text": "Architecture: Perceptron\n\n\n\n\n\ngraph LR\n    subgraph Inputs\n        x1((x1))\n        x2((x2))\n        x3((x3))\n    end\n\n    sum((Œ£))\n    act[Activation]\n    out((Output))\n    b[Bias]\n\n    x1 --&gt;|w1| sum\n    x2 --&gt;|w2| sum\n    x3 --&gt;|w3| sum\n    b --&gt; sum\n    sum --&gt; act\n    act --&gt; out\n\n    style Inputs fill:#87CEFA,stroke:#333,stroke-width:2px, fill-opacity: 0.5\n    style x1 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style x2 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style x3 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style sum fill:#FFA07A,stroke:#333,stroke-width:2px\n    style act fill:#98FB98,stroke:#333,stroke-width:2px\n    style b fill:#FFFF00,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\nInput Nodes (x1, x2, x3): Each input is a number.\nWeights (w1, w2, w3): Each weight is a number that determines the importance of the corresponding input.\nBias (b): A constant value that shifts the output of the perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-9",
    "href": "2024/weeks/week02/slides.html#section-9",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Sum Node (Œ£): Calculates the weighted sum of the inputs and the bias.\nActivation Function (\\(f\\)): Introduces non-linearity to the output of the perceptron.\nOutput Node: The final output of the perceptron.\n\n\\[\n\\text{Output} = f(w_1 \\times x_1 + w_2 \\times x_2 + w_3 \\times x_3 + b)\n\\]\n\nThe output of the perceptron is a weighted sum of the inputs and the bias passed through an activation function.\n\nWhy do we need non-linearity?\n\nNon-linearity allows the perceptron to learn complex patterns in the data.\nWithout non-linearity, the perceptron would be limited to learning linear patterns.\nActivation functions introduce non-linearity to the output of the perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#how-does-it-work",
    "href": "2024/weeks/week02/slides.html#how-does-it-work",
    "title": "Week 02 Basics of Neural Networks",
    "section": "How does it work?",
    "text": "How does it work?\n\nEach input (x1, x2, x3) is multiplied by its corresponding weight (w1, w2, w3).\nThese weighted inputs are added up with the bias (b). This is the weighted sum.\n\n(\\(w_1 \\times x_1 + w_2 \\times x_2 + w_3 \\times x_3 + b\\))\n\nThe sum is passed through an activation function.\nThe output of the activation function becomes the output of the perceptron.\nThe perceptron learns the weights and bias.\nIt compares its output to the desired output and makes corrections.\nThis process is repeated many times with all the inputs."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#additional-resources",
    "href": "2024/weeks/week02/slides.html#additional-resources",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Additional resources",
    "text": "Additional resources\n\nWhat is a neural network? [Video]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#thank-you",
    "href": "2024/weeks/week02/slides.html#thank-you",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section",
    "href": "2024/weeks/week03/slides.html#section",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "ReLU function\n\\[\nf(x) = \\max(0, x)\n\\]\nExample: \\(f(2) = 2\\)\nwhere:\n- f(x): This represents the output of the ReLU function for a given input x.\n- x: This is the input to the ReLU function.\n- max: This function returns the maximum of the two values.\n- 0: This is the threshold value.\n\nThe Rectified Linear Unit (ReLU) function is that outputs the input directly if it is positive, otherwise, it outputs zero.\nThe output of the ReLU function is between 0 and infinity.\nIt is a popular activation function used in deep learning models."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-1",
    "href": "2024/weeks/week03/slides.html#section-1",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "X-axis (Weight): Represents the value of the model parameter being optimized.\nY-axis (Loss): Represents the value of the loss function being minimized.\nThe goal is to find the value of the model parameter that minimizes the loss function."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-2",
    "href": "2024/weeks/week03/slides.html#section-2",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "The process starts at an initial weight with a corresponding loss, marked as ‚ÄúInitial weight + loss‚Äù on the graph\nGradient: The algorithm calculates the gradient (slope) at the current position. This gradient indicates the direction of steepest ascent."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-3",
    "href": "2024/weeks/week03/slides.html#section-3",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "The model then takes a step in the opposite direction of the gradient, as we want to minimize the loss.\nThis is why it‚Äôs called gradient descent - we descend along the gradient.\nThe ‚ÄúNew weight + loss‚Äù point on the graph shows an intermediate step in this process, where the loss has decreased compared to the initial position."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-4",
    "href": "2024/weeks/week03/slides.html#section-4",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "As the algorithm progresses, it should ideally approach the bottom of the curve, labeled as ‚ÄúTheoretical minima‚Äù in the image.\nThe algorithm may not always reach the exact theoretical minima due to factors like step size (learning rate) and the complexity of the loss landscape.\nBut, it typically converges to a point close enough to be practically useful for model optimization."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-5",
    "href": "2024/weeks/week03/slides.html#section-5",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "flowchart LR\n    %% Input Layer\n    I1((I1)):::inputStyle\n    I2((I2)):::inputStyle\n    I3((I3)):::inputStyle\n    B1((Bias)):::biasStyle\n    %% Hidden Layer\n    H1((H1)):::hiddenStyle\n    H2((H2)):::hiddenStyle\n    H3((H3)):::hiddenStyle\n    B2((Bias)):::biasStyle\n    %% Output Layer\n    O1((O1)):::outputStyle\n    O2((O2)):::outputStyle\n    %% Connections\n    I1 --&gt;|w11| H1\n    I1 --&gt;|w12| H2\n    I1 --&gt;|w13| H3\n    I2 --&gt;|w21| H1\n    I2 --&gt;|w22| H2\n    I2 --&gt;|w23| H3\n    I3 --&gt;|w31| H1\n    I3 --&gt;|w32| H2\n    I3 --&gt;|w33| H3\n    B1 --&gt;|b1| H1\n    B1 --&gt;|b2| H2\n    B1 --&gt;|b3| H3\n    H1 --&gt;|v11| O1\n    H1 --&gt;|v12| O2\n    H2 --&gt;|v21| O1\n    H2 --&gt;|v22| O2\n    H3 --&gt;|v31| O1\n    H3 --&gt;|v32| O2\n    B2 --&gt;|b4| O1\n    B2 --&gt;|b5| O2\n    %% Styles\n    classDef inputStyle fill:#3498db,stroke:#333,stroke-width:2px;\n    classDef hiddenStyle fill:#e74c3c,stroke:#333,stroke-width:2px;\n    classDef outputStyle fill:#2ecc71,stroke:#333,stroke-width:2px;\n    classDef biasStyle fill:#f39c12,stroke:#333,stroke-width:2px;\n    %% Layer Labels\n    I2 -.- InputLabel[Input Layer]\n    H2 -.- HiddenLabel[Hidden Layer]\n    O1 -.- OutputLabel[Output Layer]\n    style InputLabel fill:none,stroke:none\n    style HiddenLabel fill:none,stroke:none\n    style OutputLabel fill:none,stroke:none"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-6",
    "href": "2024/weeks/week03/slides.html#section-6",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "The input layer consists of three nodes (I1, I2, I3) representing the input features.\nThe hidden layer consists of three nodes (H1, H2, H3) that process the input data.\nThe output layer consists of two nodes (O1, O2) that produce the final predictions.\nThe connections between nodes are represented by weights (w11, w12, ‚Ä¶, v32) and biases (b1, b2, ‚Ä¶, b5).\nThe weights and biases are adjusted during training to optimize the model.\nThe model makes predictions by passing the input data through the network and computing the output."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#training-development-and-test-datasets",
    "href": "2024/weeks/week03/slides.html#training-development-and-test-datasets",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "Training, development and test datasets",
    "text": "Training, development and test datasets\n\nThe training dataset is used to optimize the model parameters (weights and biases) using gradient descent.\nThe development dataset is used to tune the hyperparameters of the model, such as the learning rate and the number of hidden units.\nThe test dataset is used to evaluate the performance of the model on unseen data.\nIn order to avoid overfitting, it is important to have separate datasets for training, development, and testing.\nThe training dataset is typically the largest, followed by the development and test datasets.\nThe development and test datasets should be representative of the data the model will encounter in the real world.\nThe datasets should be randomly sampled to avoid bias and ensure that the model generalizes well."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#thank-you",
    "href": "2024/weeks/week03/slides.html#thank-you",
    "title": "Week 03 Basics of Neural Networks (Part 2)",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "2024/homework/homework02.html",
    "href": "2024/homework/homework02.html",
    "title": "Homework 02: Neural Network",
    "section": "",
    "text": "Please go through the Wikipedia articles on:\n\nMatrix multiplication\nMatrix addition\nElement-wise multiplication\nDot product\n\nAnd watch this video: What is a neural network? [Video]",
    "crumbs": [
      "üß© Homework",
      "Hw 02 - NN basics  "
    ]
  },
  {
    "objectID": "2024/assignments/assignment01.html",
    "href": "2024/assignments/assignment01.html",
    "title": "Assignment 1: Perceptron Analysis",
    "section": "",
    "text": "Consider a perceptron that predicts if you‚Äôll pass a quiz based on:\n\nHours studied (0-5)\nHours slept (0-8)\n\nPart1: Make predictions for:\n\nDifferent study patterns:\n\n\nStudied 4 hours, slept 7 hours\nStudied 1 hour, slept 8 hours\nStudied 5 hours, slept 4 hours\nStudied 3 hours, slept 3 hours\nStudied 4 hours, slept 4 hours\n\n\nWeight changes:\n\n\nStudy hours more important\nSleep hours more important\nBoth equally important\n\nDraw simple diagrams showing:\n\nHow you think the perceptron would classify each case\nWhy some combinations might work better than others\n\nPart 2: Predict what happens when:\n\nThe perceptron starts with:\n\n\nThinking only study hours matter\nThinking only sleep matters\nThinking nothing matters (zero weights)\n\n\nThe perceptron learns from:\n\n\nOne student‚Äôs data\nFive students data\nTwenty students data\n\nExplain in simple terms:\n\nHow you think it would change its decisions\nWhy more data might help\nWhat problems it might face\n\nNOTE:\n\nNo calculations needed\nKeep explanations simple\nYou will be graded on:\n\nclarity of explanation\nuse of diagrams to support your explanation\n\n\nSubmission Guidelines\n\nFormat: PDF document, 12-point Times New Roman, 1.5 line spacing\nLength: 600-1500 words (excluding diagrams)\nSubmit by email to: Anna (anste145 at hhu dot de)\nDeadline: 6th November 2024, 23:59 CET",
    "crumbs": [
      "‚úçÔ∏è Assignments   ",
      "üìù Assignment 01"
    ]
  },
  {
    "objectID": "2024/weeks/week04/page.html",
    "href": "2024/weeks/week04/page.html",
    "title": "üóìÔ∏è Week 04 - Intro to tranformers",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 04 - Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week04/page.html#lecture-slides",
    "href": "2024/weeks/week04/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 04 - Intro to tranformers",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 04 - Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week04/page.html#homework",
    "href": "2024/weeks/week04/page.html#homework",
    "title": "üóìÔ∏è Week 04 - Intro to tranformers",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\n\nReading\n\nRead the first 5 pages of Attention is All You Need paper.\nRead about Self-attention mechanism here: https://jalammar.github.io/illustrated-transformer/\nWatch these videos:\n\nhttps://www.youtube.com/watch?v=wjZofJX0v4M\nhttps://www.youtube.com/watch?v=eMlx5fFNoYc",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 04 - Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week05/page.html",
    "href": "2024/weeks/week05/page.html",
    "title": "üóìÔ∏è Week 05 - Intro to tranformers (part 2)",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 05 - Transformers (part 2)"
    ]
  },
  {
    "objectID": "2024/weeks/week05/page.html#lecture-slides",
    "href": "2024/weeks/week05/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 05 - Intro to tranformers (part 2)",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 05 - Transformers (part 2)"
    ]
  },
  {
    "objectID": "2024/weeks/week05/page.html#homework",
    "href": "2024/weeks/week05/page.html#homework",
    "title": "üóìÔ∏è Week 05 - Intro to tranformers (part 2)",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\n\nReading\n\nRead the first 8 pages of Attention is All You Need paper.\nRead about Self-attention mechanism here: https://jalammar.github.io/illustrated-transformer/\nWatch these videos:\n\nHow large language models work, a visual intro to transformers\nAttention in transformers, visually explained\nIllustrated Guide to Transformers Neural Network: A step by step explanation",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 05 - Transformers (part 2)"
    ]
  },
  {
    "objectID": "2024/weeks/week06/page.html",
    "href": "2024/weeks/week06/page.html",
    "title": "üóìÔ∏è Week 06 - Hands-on with Transformers",
    "section": "",
    "text": "Click here to download the data preprocessing notebook.\nClick here to download the self-attention notebook.\nOpen these two notebooks in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 06 - Hands-on with Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week06/page.html#lecture",
    "href": "2024/weeks/week06/page.html#lecture",
    "title": "üóìÔ∏è Week 06 - Hands-on with Transformers",
    "section": "",
    "text": "Click here to download the data preprocessing notebook.\nClick here to download the self-attention notebook.\nOpen these two notebooks in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 06 - Hands-on with Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week06/page.html#homework",
    "href": "2024/weeks/week06/page.html#homework",
    "title": "üóìÔ∏è Week 06 - Hands-on with Transformers",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 06 - Hands-on with Transformers (part 1)"
    ]
  },
  {
    "objectID": "2024/weeks/week08/page.html",
    "href": "2024/weeks/week08/page.html",
    "title": "üóìÔ∏è Week 08 - Finetuning GPT-2 model for text classification",
    "section": "",
    "text": "Click here to download the implementation of GPT-2 model.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 08 - finetuning text classification"
    ]
  },
  {
    "objectID": "2024/weeks/week08/page.html#lecture",
    "href": "2024/weeks/week08/page.html#lecture",
    "title": "üóìÔ∏è Week 08 - Finetuning GPT-2 model for text classification",
    "section": "",
    "text": "Click here to download the implementation of GPT-2 model.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 08 - finetuning text classification"
    ]
  },
  {
    "objectID": "2024/weeks/week08/page.html#homework",
    "href": "2024/weeks/week08/page.html#homework",
    "title": "üóìÔ∏è Week 08 - Finetuning GPT-2 model for text classification",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 08 - finetuning text classification"
    ]
  },
  {
    "objectID": "2024/weeks/week10/page.html",
    "href": "2024/weeks/week10/page.html",
    "title": "üóìÔ∏è Week 10 - Morphological inflection task using T5",
    "section": "",
    "text": "Click here to download the notebook.\nTo know more about Morphological inflection, check this file.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 10 - morphological inflection using T5"
    ]
  },
  {
    "objectID": "2024/weeks/week10/page.html#lecture",
    "href": "2024/weeks/week10/page.html#lecture",
    "title": "üóìÔ∏è Week 10 - Morphological inflection task using T5",
    "section": "",
    "text": "Click here to download the notebook.\nTo know more about Morphological inflection, check this file.\nOpen the notebook in google colab.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 10 - morphological inflection using T5"
    ]
  },
  {
    "objectID": "2024/weeks/week10/page.html#homework",
    "href": "2024/weeks/week10/page.html#homework",
    "title": "üóìÔ∏è Week 10 - Morphological inflection task using T5",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 10 - morphological inflection using T5"
    ]
  },
  {
    "objectID": "2024/weeks/week13/page.html",
    "href": "2024/weeks/week13/page.html",
    "title": "üóìÔ∏è Week 13 - Other finetuning methods",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 13 - Other finetuning methods"
    ]
  },
  {
    "objectID": "2024/weeks/week13/page.html#lecture-slides",
    "href": "2024/weeks/week13/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 13 - Other finetuning methods",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 13 - Other finetuning methods"
    ]
  }
]