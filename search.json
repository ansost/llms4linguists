[
  {
    "objectID": "2024/homework/homework01.html",
    "href": "2024/homework/homework01.html",
    "title": "Assignment 1: Using Google Colab",
    "section": "",
    "text": "The instructions below assume that you have a Google account or are willing to make one. If you do not want to make a Google account,  contact Anna for instructions to run notebooks without Colab.",
    "crumbs": [
      "🧩 Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/homework/homework01.html#google-colab",
    "href": "2024/homework/homework01.html#google-colab",
    "title": "Assignment 1: Using Google Colab",
    "section": "Google Colab",
    "text": "Google Colab\n\nGo to the notebooks folder. Click on the three dots for the hw1.ipynb file, select “Open with” and then “Open in Colab”.\nRun both cells in the notebook by clicking on the play icon on the left of the cell or clicking into the cell and pressing CTRL + ENTER on Windows or CMD + ENTER on Mac.",
    "crumbs": [
      "🧩 Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html",
    "href": "2024/weeks/week01/page.html",
    "title": "🗓️ Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.",
    "crumbs": [
      "🗓️ Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#lecture-slides",
    "href": "2024/weeks/week01/page.html#lecture-slides",
    "title": "🗓️ Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "🗓️ Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#coursework",
    "href": "2024/weeks/week01/page.html#coursework",
    "title": "🗓️ Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n🚧 Come back after the lecture to read the coursework instructions for the week 🚧",
    "crumbs": [
      "🗓️ Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#recommended-reading",
    "href": "2024/weeks/week01/page.html#recommended-reading",
    "title": "🗓️ Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "📚 Recommended Reading",
    "text": "📚 Recommended Reading\n\nSpeech and Language Processing, Chapter 3: N-gram language models PDF. Authors: Dan Jurafsky and James H. Martin.",
    "crumbs": [
      "🗓️ Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/homework.html",
    "href": "2024/homework.html",
    "title": "Homework Overview",
    "section": "",
    "text": "Here is an overview of all Homework up to date:\nHw 1: here\nTBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLMs in Linguistic Research",
    "section": "",
    "text": "InstructorTeaching Staff\n\n\n\nAkhilesh Kakolu Ramarao  📧 kakolura at hhu dot de\n\n\n\nAnna Stein  📧 anna.stein at hhu dot de\n\n\n\n\nSyllabus\nView it online here.\nDownload link: Click!\n\n\n📍 Location\nWednesday, 2:30pm-4pm\nBuilding 23.21 Floor 2, Room 22\n\n\n🎯 Learning Objectives\nFocus: Understanding LLMs and their usage in various linguistics tasks\nHow: Hands-on learning and paper discussions\n\nGaining an intuition of how LLMs work\nGetting to know the field of LLM research and development\nFamiliarizing yourself with different LLM architectures\nLearning how to use them for different linguistics tasks (e.g., text analysis, language generation, and language understanding)\nPracting using tools like Hugginface for LLM inference and fine-tuning",
    "crumbs": [
      "🏠 Home"
    ]
  },
  {
    "objectID": "2024/assignments.html",
    "href": "2024/assignments.html",
    "title": "Assignment Overview",
    "section": "",
    "text": "Here is an overview of all assignments up to date:\nAssignment 1: TBD\nAssignment 2: TBD\nAssignment 3: TBD\nAssignment 4: TBD\nAssignment 5: TBD"
  },
  {
    "objectID": "2024/syllabus.html",
    "href": "2024/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Last changed: 09.10.24\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Recurrent Neural Networks\n\nAssignment 1\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD",
    "crumbs": [
      "📓 Syllabus"
    ]
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "href": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Akhilesh",
    "text": "WhoamI: Akhilesh\n\n\n\n\n\nAkhilesh Kakolu Ramarao (he/him) PhD researcher at the English Language and Linguistics department\n\n\n\n\nResearching Computational Morphology supervised by Prof. Dr. Kevin Tang and Dr. Dinah Baer-Henney\nStarted at HHU in 2021\nIndustry Background: NLP Researcher, Software Engineer, Building multilingual chatbots, voice assistants\nInvolved in several language revitalization efforts for indigenous communities in Arunachal Pradesh, India, with a focus on Idu Mishmi and K’man Mishmi languages\nMore: https://akkikek.xyz/about.html\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-anna",
    "href": "2024/weeks/week01/slides.html#whoami-anna",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Anna",
    "text": "WhoamI: Anna\n\n\n\n\n\nAnna Sophia Stein (she/her)  MSc Linguistics student Focus on Computational Linguistics 📧 anna.stein@hhu.de \n\n\n\n\nMain interests: anything Natural Language Processing, open source, …\nPart of research at the Anglistics, Linguistics and Computer Science department\nCurrently writing my MA thesis on making LLMs better at pragmatics\n\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#contact",
    "href": "2024/weeks/week01/slides.html#contact",
    "title": "Week 01 Introduction",
    "section": "Contact",
    "text": "Contact\n\n📧: kakolura@hhu.de\n🏢🕰️: By appointment\n🌐👥: https://slam.phil.hhu.de\n🐦: SLaMLab_HHU\n🐙🐱: https://github.com/hhuslamlab/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview",
    "href": "2024/weeks/week01/slides.html#syllabus-overview",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Recurrent Neural Networks\n\nAssignment 1"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#statistical-language-models",
    "href": "2024/weeks/week01/slides.html#statistical-language-models",
    "title": "Week 01 Introduction",
    "section": "Statistical Language Models",
    "text": "Statistical Language Models\n\nThese models use statistical patterns in the data to make predictions about the likelihood of specific sequences of words\nN-gram models are the most common type of statistical language model that predicts the probability of a word given the previous n-1 words\nThey are simple and computationally efficient but have limitations in capturing long-range dependencies\nThey are widely used in speech recognition, machine translation, and other NLP tasks\nThey are used as a baseline for more complex language models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "href": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "title": "Week 01 Introduction",
    "section": "Probabilistic Language Models",
    "text": "Probabilistic Language Models\n\nThese models assign a probability to sequences of words based on the training data\nThey are based on the principles of probability theory and use probabilistic methods to model the language\nThey can capture complex patterns in the data and are more flexible than statistical models\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and speech recognition"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#neural-language-models",
    "href": "2024/weeks/week01/slides.html#neural-language-models",
    "title": "Week 01 Introduction",
    "section": "Neural Language Models",
    "text": "Neural Language Models\n\nThese models use neural networks to predict the likelihood of a sequence of words\nThey are trained on a large corpus of text data and can learn complex patterns and dependencies\nThey are more powerful than traditional statistical and probabilistic models\nThey are used in a wide range of NLP tasks, such as, machine translation, text generation, and sentiment analysis"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#large-language-models",
    "href": "2024/weeks/week01/slides.html#large-language-models",
    "title": "Week 01 Introduction",
    "section": "Large Language Models",
    "text": "Large Language Models\n\nThey are advanced language models that handle billions of training data parameters and generate text output\nThey are trained on large-scale datasets and can generate human-like text\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and question answering\nThey are the focus of the course"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#thank-you",
    "href": "2024/weeks/week01/slides.html#thank-you",
    "title": "Week 01 Introduction",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\nLLMs in Lingustic Research WiSe 2024/25"
  }
]