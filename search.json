[
  {
    "objectID": "2024/weeks/week03/slides.html#activation-functions",
    "href": "2024/weeks/week03/slides.html#activation-functions",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Activation functions",
    "text": "Activation functions\n\nActivation functions are used to introduce non-linearity to the output of a neuron.\n\nSigmoid function \\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nExample: \\(f(0) = 0.5\\) - f(x): This represents the output of the sigmoid function for a given input x. - e: This is the euler‚Äôs number (approximately 2.71828). - x: This is the input to the sigmoid function. - 1: This is added to the denominator to avoid division by zero.\n\nThe sigmoid function takes any real number as input and outputs a value between 0 and 1.\nIt is used in the output layer of a binary classification problem."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section",
    "href": "2024/weeks/week03/slides.html#section",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "ReLU function\n\\[\nf(x) = \\max(0, x)\n\\]\nExample: \\(f(2) = 2\\)\nwhere: - f(x): This represents the output of the ReLU function for a given input x. - x: This is the input to the ReLU function. - max: This function returns the maximum of the two values. - 0: This is the threshold value.\n\nThe Rectified Linear Unit (ReLU) function is that outputs the input directly if it is positive, otherwise, it outputs zero.\nThe output of the ReLU function is between 0 and infinity.\nIt is a popular activation function used in deep learning models."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#loss-functions",
    "href": "2024/weeks/week03/slides.html#loss-functions",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Loss functions",
    "text": "Loss functions\n\nDuring forward pass, the neural network makes predictions based on input data.\nThe loss function compares these predictions to the true values and calculates a loss score.\nThe loss score is a measure of how well the network is performing.\nThe goal of training is to minimize the loss function.\nFor regression problems, use MSE or MAE.\nFor classification problems, use cross-entropy loss.\nFor multi-class classification problems, use categorical cross-entropy loss."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#gradient-descent",
    "href": "2024/weeks/week03/slides.html#gradient-descent",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Gradient descent",
    "text": "Gradient descent\n\nGradient descent is a optimization algorithm used in machine learning to minimize the loss function of a model."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-1",
    "href": "2024/weeks/week03/slides.html#section-1",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "X-axis (Weight): Represents the value of the model parameter being optimized.\nY-axis (Loss): Represents the value of the loss function being minimized.\nThe goal is to find the value of the model parameter that minimizes the loss function."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-2",
    "href": "2024/weeks/week03/slides.html#section-2",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "The process starts at an initial weight with a corresponding loss, marked as ‚ÄúInitial weight + loss‚Äù on the graph\nGradient: The algorithm calculates the gradient (slope) at the current position. This gradient indicates the direction of steepest ascent."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-3",
    "href": "2024/weeks/week03/slides.html#section-3",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "The model then takes a step in the opposite direction of the gradient, as we want to minimize the loss.\nThis is why it‚Äôs called gradient descent - we descend along the gradient.\nThe ‚ÄúNew weight + loss‚Äù point on the graph shows an intermediate step in this process, where the loss has decreased compared to the initial position."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-4",
    "href": "2024/weeks/week03/slides.html#section-4",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "As the algorithm progresses, it should ideally approach the bottom of the curve, labeled as ‚ÄúTheoretical minima‚Äù in the image.\nThe algorithm may not always reach the exact theoretical minima due to factors like step size (learning rate) and the complexity of the loss landscape.\nBut, it typically converges to a point close enough to be practically useful for model optimization."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#learning-rate",
    "href": "2024/weeks/week03/slides.html#learning-rate",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Learning rate",
    "text": "Learning rate\n\nThe learning rate is a hyperparameter that controls how much the model parameters are adjusted during training.\nIt is a critical parameter that can affect the convergence of the optimization algorithm.\nA high learning rate can cause the model to overshoot the minima, leading to instability and divergence.\nA low learning rate can slow down the training process and may get stuck in local minima."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#neural-network",
    "href": "2024/weeks/week03/slides.html#neural-network",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Neural Network",
    "text": "Neural Network\n\nA neural network is a collection of interconnected nodes (neurons) that process input data to produce output predictions.\nThe nodes are organized into layers, with each layer performing specific computations.\nThe input layer receives the input data, the hidden layers process the data, and the output layer produces the final predictions.\nThe connections between nodes are represented by weights, which are adjusted during training to optimize the model."
  },
  {
    "objectID": "2024/weeks/week03/slides.html#section-5",
    "href": "2024/weeks/week03/slides.html#section-5",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "",
    "text": "flowchart LR\n    %% Input Layer\n    I1((I1)):::inputStyle\n    I2((I2)):::inputStyle\n    I3((I3)):::inputStyle\n    B1((Bias)):::biasStyle\n    %% Hidden Layer\n    H1((H1)):::hiddenStyle\n    H2((H2)):::hiddenStyle\n    H3((H3)):::hiddenStyle\n    B2((Bias)):::biasStyle\n    %% Output Layer\n    O1((O1)):::outputStyle\n    O2((O2)):::outputStyle\n    %% Connections\n    I1 --&gt;|w11| H1\n    I1 --&gt;|w12| H2\n    I1 --&gt;|w13| H3\n    I2 --&gt;|w21| H1\n    I2 --&gt;|w22| H2\n    I2 --&gt;|w23| H3\n    I3 --&gt;|w31| H1\n    I3 --&gt;|w32| H2\n    I3 --&gt;|w33| H3\n    B1 --&gt;|b1| H1\n    B1 --&gt;|b2| H2\n    B1 --&gt;|b3| H3\n    H1 --&gt;|v11| O1\n    H1 --&gt;|v12| O2\n    H2 --&gt;|v21| O1\n    H2 --&gt;|v22| O2\n    H3 --&gt;|v31| O1\n    H3 --&gt;|v32| O2\n    B2 --&gt;|b4| O1\n    B2 --&gt;|b5| O2\n    %% Styles\n    classDef inputStyle fill:#3498db,stroke:#333,stroke-width:2px;\n    classDef hiddenStyle fill:#e74c3c,stroke:#333,stroke-width:2px;\n    classDef outputStyle fill:#2ecc71,stroke:#333,stroke-width:2px;\n    classDef biasStyle fill:#f39c12,stroke:#333,stroke-width:2px;\n    %% Layer Labels\n    I2 -.- InputLabel[Input Layer]\n    H2 -.- HiddenLabel[Hidden Layer]\n    O1 -.- OutputLabel[Output Layer]\n    style InputLabel fill:none,stroke:none\n    style HiddenLabel fill:none,stroke:none\n    style OutputLabel fill:none,stroke:none"
  },
  {
    "objectID": "2024/weeks/week03/slides.html#thank-you",
    "href": "2024/weeks/week03/slides.html#thank-you",
    "title": "Week 02 Basics of Neural Networks (Part 2)",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\nLLMs in Lingustic Research WiSe 2024/25"
  },
  {
    "objectID": "2024/homework/homework02.html",
    "href": "2024/homework/homework02.html",
    "title": "Homework 02: Neural Network",
    "section": "",
    "text": "Please go through the Wikipedia articles on:\n\nMatrix multiplication\nMatrix addition\nElement-wise multiplication\nDot product\n\nAnd watch this video: What is a neural network? [Video]",
    "crumbs": [
      "üß© Homework",
      "Hw 02 - NN basics  "
    ]
  },
  {
    "objectID": "2024/weeks/week03/page.html",
    "href": "2024/weeks/week03/page.html",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course."
  },
  {
    "objectID": "2024/weeks/week03/page.html#lecture-slides",
    "href": "2024/weeks/week03/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "2024/weeks/week03/page.html#homework",
    "href": "2024/weeks/week03/page.html#homework",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\nWeek 01 - Homework"
  },
  {
    "objectID": "2024/weeks/week03/page.html#recommended-reading",
    "href": "2024/weeks/week03/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nSpeech and Language Processing, Chapter 3: N-gram language models PDF. Authors: Dan Jurafsky and James H. Martin."
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "href": "2024/weeks/week01/slides.html#whoami-akhilesh",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Akhilesh",
    "text": "WhoamI: Akhilesh\n\n\n\n\n\nAkhilesh Kakolu Ramarao (he/him) PhD researcher at the English Language and Linguistics department\n\n\n\n\nResearching Computational Morphology supervised by Prof.¬†Dr.¬†Kevin Tang and Dr.¬†Dinah Baer-Henney\nStarted at HHU in 2021\nIndustry Background: NLP Researcher, Software Engineer, Building multilingual chatbots, voice assistants\nInvolved in several language revitalization efforts for indigenous communities in Arunachal Pradesh, India, with a focus on Idu Mishmi and K‚Äôman Mishmi languages\nMore: https://akkikek.xyz/about.html\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#whoami-anna",
    "href": "2024/weeks/week01/slides.html#whoami-anna",
    "title": "Week 01 Introduction",
    "section": "WhoamI: Anna",
    "text": "WhoamI: Anna\n\n\n\n\n\nAnna Sophia Stein (she/her)  MSc Linguistics student Focus on Computational Linguistics üìß anna.stein@hhu.de \n\n\n\n\nMain interests: anything Natural Language Processing, open source, ‚Ä¶\nPart of research at the Anglistics, Linguistics and Computer Science department\nCurrently writing my MA thesis on making LLMs better at pragmatics\n\nPart of the Slamlab: https://slam.phil.hhu.de/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#contact",
    "href": "2024/weeks/week01/slides.html#contact",
    "title": "Week 01 Introduction",
    "section": "Contact",
    "text": "Contact\n\nüìß: kakolura@hhu.de\nüè¢üï∞Ô∏è: By appointment\nüåêüë•: https://slam.phil.hhu.de\nüê¶: SLaMLab_HHU\nüêôüê±: https://github.com/hhuslamlab/"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview",
    "href": "2024/weeks/week01/slides.html#syllabus-overview",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Recurrent Neural Networks\n\nAssignment 1"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-1",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "href": "2024/weeks/week01/slides.html#syllabus-overview-2",
    "title": "Week 01 Introduction",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#statistical-language-models",
    "href": "2024/weeks/week01/slides.html#statistical-language-models",
    "title": "Week 01 Introduction",
    "section": "Statistical Language Models",
    "text": "Statistical Language Models\n\nThese models use statistical patterns in the data to make predictions about the likelihood of specific sequences of words\nN-gram models are the most common type of statistical language model that predicts the probability of a word given the previous n-1 words\nThey are simple and computationally efficient but have limitations in capturing long-range dependencies\nThey are widely used in speech recognition, machine translation, and other NLP tasks\nThey are used as a baseline for more complex language models"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "href": "2024/weeks/week01/slides.html#probabilistic-language-models",
    "title": "Week 01 Introduction",
    "section": "Probabilistic Language Models",
    "text": "Probabilistic Language Models\n\nThese models assign a probability to sequences of words based on the training data\nThey are based on the principles of probability theory and use probabilistic methods to model the language\nThey can capture complex patterns in the data and are more flexible than statistical models\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and speech recognition"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#neural-language-models",
    "href": "2024/weeks/week01/slides.html#neural-language-models",
    "title": "Week 01 Introduction",
    "section": "Neural Language Models",
    "text": "Neural Language Models\n\nThese models use neural networks to predict the likelihood of a sequence of words\nThey are trained on a large corpus of text data and can learn complex patterns and dependencies\nThey are more powerful than traditional statistical and probabilistic models\nThey are used in a wide range of NLP tasks, such as, machine translation, text generation, and sentiment analysis"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#large-language-models",
    "href": "2024/weeks/week01/slides.html#large-language-models",
    "title": "Week 01 Introduction",
    "section": "Large Language Models",
    "text": "Large Language Models\n\nThey are advanced language models that handle billions of training data parameters and generate text output\nThey are trained on large-scale datasets and can generate human-like text\nThey are used in a wide range of NLP tasks, such as machine translation, text generation, and question answering\nThey are the focus of the course"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#thank-you",
    "href": "2024/weeks/week01/slides.html#thank-you",
    "title": "Week 01 Introduction",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\nLLMs in Lingustic Research WiSe 2024/25"
  },
  {
    "objectID": "2024/syllabus.html",
    "href": "2024/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Last changed: 23.10.24\n\n\n\n\n\n\n\n\n\nDate\nGeneral Topic\nHomework\nAssignments\n\n\n\n\n09.10\nAdmin, Architectures: Statistical and Probabilistic Language Models\nFamiliarization with Google Colab\n\n\n\n16.10\nArchitectures: Perceptrons and Neural Networks\n\n\n\n\n23.10\nArchitectures: Perceptrons and Neural Networks (part 2)\n\nAssignment 1\n\n\n30.10\nTransformer: General architecture\n\n\n\n\n06.11\nTransformer: The Attention mechanism\n\n\n\n\n13.11\nTransformer: Transformer models: Decoder/Encode only models\n\nAssignment 2\n\n\n20.11\nUsing pre-trained models\n\n\n\n\n27.11\nStudy week\n\n\n\n\n04.12\nTransfer learning: fine-tuning\n\n\n\n\n11.12\nAdapting models for specific tasks\n\nAssignment 3\n\n\n18.12\nAdapting models for specific tasks\n\n\n\n\n08.01\nAdapting models for specific tasks\n\nAssignment 4\n\n\n15.01\nProbing LLMs\n\n\n\n\n22.01\nProbing LLMs\n\nAssignment 5\n\n\n29.01\nTBD",
    "crumbs": [
      "üìì Syllabus"
    ]
  },
  {
    "objectID": "2024/assignments.html",
    "href": "2024/assignments.html",
    "title": "Assignment Overview",
    "section": "",
    "text": "Here is an overview of all assignments up to date:\nAssignment 1: TBD\nAssignment 2: TBD\nAssignment 3: TBD\nAssignment 4: TBD\nAssignment 5: TBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLMs in Linguistic Research",
    "section": "",
    "text": "InstructorTeaching Staff\n\n\n Akhilesh Kakolu Ramarao  üìß kakolura at hhu dot de\n\n\n Anna Stein  üìß anna.stein at hhu dot de\n\n\n\n\nSyllabus\nView it online here.\n\n\nüìç Location\nWednesday, 2:30pm-4pm\nBuilding 23.21 Floor 2, Room 22\n\n\nüéØ Learning Objectives\nFocus: Understanding LLMs and their usage in various linguistics tasks\nHow: Hands-on learning and paper discussions\n\nGaining an intuition of how LLMs work\nGetting to know the field of LLM research and development\nFamiliarizing yourself with different LLM architectures\nLearning how to use them for different linguistics tasks (e.g., text analysis, language generation, and language understanding)\nPracting using tools like Huggingface for LLM inference and fine-tuning",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "2024/homework.html",
    "href": "2024/homework.html",
    "title": "Homework Overview",
    "section": "",
    "text": "Here is an overview of all Homework up to date:\nHw 1: Google Colab: here\nHw 2: Neural Networks: here"
  },
  {
    "objectID": "2024/weeks/week01/page.html",
    "href": "2024/weeks/week01/page.html",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#lecture-slides",
    "href": "2024/weeks/week01/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#homework",
    "href": "2024/weeks/week01/page.html#homework",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\nWeek 01 - Homework",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#recommended-reading",
    "href": "2024/weeks/week01/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 01 - Admin, Architectures: Statistical and Probabilistic Language Models",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nSpeech and Language Processing, Chapter 3: N-gram language models PDF. Authors: Dan Jurafsky and James H. Martin.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 01 - Intro"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html",
    "href": "2024/weeks/week02/page.html",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "",
    "text": "In this second week, we will cover a few basic math concepts used in neural networks, try to get an intuition of what machine learning models are and finally explain the architecture a perceptron.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#lecture-slides",
    "href": "2024/weeks/week02/page.html#lecture-slides",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#homework",
    "href": "2024/weeks/week02/page.html#homework",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "‚úçÔ∏è Homework",
    "text": "‚úçÔ∏è Homework\n\nPlease go through the slides and the recommended reading material.",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page.html#recommended-reading",
    "href": "2024/weeks/week02/page.html#recommended-reading",
    "title": "üóìÔ∏è Week 02 - Basics of Neural Networks",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nWikipedia articles on Matrix multiplication, matrix addition, element-wise multiplication, and dot product.\nWhat is a neural network? [Video]",
    "crumbs": [
      "üóìÔ∏è Class material",
      "Week 02 - NN basics"
    ]
  },
  {
    "objectID": "2024/homework/homework01.html",
    "href": "2024/homework/homework01.html",
    "title": "Homework 1: Using Google Colab",
    "section": "",
    "text": "The instructions below assume that you have a Google account or are willing to make one. If you do not want to make a Google account,  contact Anna for instructions to run notebooks without Colab.",
    "crumbs": [
      "üß© Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/homework/homework01.html#google-colab",
    "href": "2024/homework/homework01.html#google-colab",
    "title": "Homework 1: Using Google Colab",
    "section": "Google Colab",
    "text": "Google Colab\n\nGo to the notebooks folder. Click on the three dots for the hw1.ipynb file, select ‚ÄúOpen with‚Äù and then ‚ÄúOpen in Colab‚Äù.\nRun both cells in the notebook by clicking on the play icon on the left of the cell or clicking into the cell and pressing CTRL + ENTER on Windows or CMD + ENTER on Mac.",
    "crumbs": [
      "üß© Homework",
      "Hw 01 - Colab"
    ]
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section",
    "href": "2024/weeks/week02/slides.html#section",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Scalars: single number\n\\[\nx = 1\n\\]\nVectors: sequence of numbers\n\\[\nv = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nMatrix: 2D list of numbers\n\\[\nM = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-1",
    "href": "2024/weeks/week02/slides.html#section-1",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Matrix multiplication\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 22 & 28 \\\\ 49 & 64 \\end{bmatrix}\n\\]\nExplanation:\n\nThe first matrix has 2 rows and 3 columns, and the second matrix has 3 rows and 2 columns.\nThe number of columns in the first matrix should be equal to the number of rows in the second matrix.\nThe resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\nYou multiply the rows of the first matrix with the columns of the second matrix."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-2",
    "href": "2024/weeks/week02/slides.html#section-2",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Element-wise multiplication\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\odot \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 1 & 4 & 9 \\\\ 16 & 25 & 36 \\end{bmatrix}\n\\]\n\nThe matrices should have the same dimensions.\nThe resulting matrix will have the same dimensions as the input matrices.\nYou multiply the corresponding elements of the matrices."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-3",
    "href": "2024/weeks/week02/slides.html#section-3",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Matrix addition\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n\\]\n\\[\n= \\begin{bmatrix} 2 & 4 & 6 \\\\ 8 & 10 & 12 \\end{bmatrix}\n\\]\n\nYou add the corresponding elements of the matrices.\nThe matrices should have the same dimensions.\nThe resulting matrix will have the same dimensions as the input matrices."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-4",
    "href": "2024/weeks/week02/slides.html#section-4",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Dot product\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\n\\[\n= 1 \\times 1 + 2 \\times 2 + 3 \\times 3 = 14\n\\]\n\nThe number of columns in the first matrix should be equal to the number of rows in the second matrix.\nThe resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\nYou multiply the corresponding elements of the matrices and sum them up."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-5",
    "href": "2024/weeks/week02/slides.html#section-5",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Let us say that you are given a set of inputs and outputs. You need to find how the inputs are related to the outputs.\nInputs: \\(0,1,2,3,4\\)\nOutputs: \\(0,2,4,6,8\\)\n\nYou can see that the output is twice the input.\nThis is a simple example of a relationship between inputs and outputs.\nYou can use this relationship to predict the output for new inputs.\n\nConsider a more complex relationship between inputs and outputs.\nInputs: \\(0,1,2,3,4\\)\nOutputs: \\(0,1,1,2,3\\)\n\nCan you find the relationship between the inputs and outputs?\nThis is where machine learning comes into play.\nMachine learning algorithms can learn the relationship between inputs and outputs from the data."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#neural-networks",
    "href": "2024/weeks/week02/slides.html#neural-networks",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nNeural networks are a class of machine learning models inspired by the human brain.\n\nLearning Alorithm\n\nNeural networks learn by looking at many examples.\nThey adjust their internal settings (= parameters) to improve their accuracy.\nThis is process is called training.\n\nAdvantages of neural networks\n\nCan learn complex patterns.\nCan generalize to new data.\nCan be used for a wide range of tasks (speech recognition, and natural language processing)."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#architecture",
    "href": "2024/weeks/week02/slides.html#architecture",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Architecture",
    "text": "Architecture\n   The input can be a vector, and the output some classification, like a corresponding animal."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-6",
    "href": "2024/weeks/week02/slides.html#section-6",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Every Neural Network has Layers. They are responsible for a specific action, like addition, and pass information to eachother."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-7",
    "href": "2024/weeks/week02/slides.html#section-7",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Layers consist of neurons which each modify the input in some way."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-8",
    "href": "2024/weeks/week02/slides.html#section-8",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "The simplest Neural network only has one layer with one neuron. This is called a perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#architecture-perceptron",
    "href": "2024/weeks/week02/slides.html#architecture-perceptron",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Architecture: Perceptron",
    "text": "Architecture: Perceptron\n\n\n\n\n\ngraph LR\n    subgraph Inputs\n        x1((x1))\n        x2((x2))\n        x3((x3))\n    end\n\n    sum((Œ£))\n    act[Activation]\n    out((Output))\n    b[Bias]\n\n    x1 --&gt;|w1| sum\n    x2 --&gt;|w2| sum\n    x3 --&gt;|w3| sum\n    b --&gt; sum\n    sum --&gt; act\n    act --&gt; out\n\n    style Inputs fill:#87CEFA,stroke:#333,stroke-width:2px, fill-opacity: 0.5\n    style x1 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style x2 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style x3 fill:#87CEFA,stroke:#333,stroke-width:2px\n    style sum fill:#FFA07A,stroke:#333,stroke-width:2px\n    style act fill:#98FB98,stroke:#333,stroke-width:2px\n    style b fill:#FFFF00,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\nInput Nodes (x1, x2, x3): Each input is a number.\nWeights (w1, w2, w3): Each weight is a number that determines the importance of the corresponding input.\nBias (b): A constant value that shifts the output of the perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#section-9",
    "href": "2024/weeks/week02/slides.html#section-9",
    "title": "Week 02 Basics of Neural Networks",
    "section": "",
    "text": "Sum Node (Œ£): Calculates the weighted sum of the inputs and the bias.\nActivation Function (\\(f\\)): Introduces non-linearity to the output of the perceptron.\nOutput Node: The final output of the perceptron.\n\n\\[\n\\text{Output} = f(w_1 \\times x_1 + w_2 \\times x_2 + w_3 \\times x_3 + b)\n\\]\n\nThe output of the perceptron is a weighted sum of the inputs and the bias passed through an activation function.\n\nWhy do we need non-linearity?\n\nNon-linearity allows the perceptron to learn complex patterns in the data.\nWithout non-linearity, the perceptron would be limited to learning linear patterns.\nActivation functions introduce non-linearity to the output of the perceptron."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#how-does-it-work",
    "href": "2024/weeks/week02/slides.html#how-does-it-work",
    "title": "Week 02 Basics of Neural Networks",
    "section": "How does it work?",
    "text": "How does it work?\n\nEach input (x1, x2, x3) is multiplied by its corresponding weight (w1, w2, w3).\nThese weighted inputs are added up with the bias (b). This is the weighted sum.\n\n(\\(w_1 \\times x_1 + w_2 \\times x_2 + w_3 \\times x_3 + b\\))\n\nThe sum is passed through an activation function.\nThe output of the activation function becomes the output of the perceptron.\nThe perceptron learns the weights and bias.\nIt compares its output to the desired output and makes corrections.\nThis process is repeated many times with all the inputs."
  },
  {
    "objectID": "2024/weeks/week02/slides.html#additional-resources",
    "href": "2024/weeks/week02/slides.html#additional-resources",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Additional resources",
    "text": "Additional resources\n\nWhat is a neural network? [Video]"
  },
  {
    "objectID": "2024/weeks/week02/slides.html#thank-you",
    "href": "2024/weeks/week02/slides.html#thank-you",
    "title": "Week 02 Basics of Neural Networks",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\n\nLLMs in Lingustic Research WiSe 2024/25"
  }
]